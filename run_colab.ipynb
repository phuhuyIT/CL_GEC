{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac4ccfc",
   "metadata": {},
   "source": [
    "# üáªüá≥ Vietnamese GEC with Contrastive Learning - Google Colab (BARTpho Fixed)\n",
    "\n",
    "**‚úÖ Bug Fix Applied:** Fixed `'BartphoTokenizer' object has no attribute 'vocab'` error\n",
    "\n",
    "Complete pipeline for training Vietnamese Grammatical Error Correction models with Contrastive Learning.\n",
    "\n",
    "## üêõ Recent Fixes:\n",
    "- **BARTpho Tokenizer Fix**: Resolved vocabulary access issue for SentencePiece tokenizers\n",
    "- **Improved Compatibility**: Better support for both BARTpho and ViT5 models\n",
    "- **Error Handling**: Added safe vocabulary checking methods\n",
    "\n",
    "## üìã Pipeline Overview:\n",
    "1. **Setup & Installation** - Install dependencies and create project structure\n",
    "2. **Data Preparation** - Load and preprocess viGEC dataset  \n",
    "3. **Base Model Training** - Fine-tune BARTpho/ViT5 with hyperparameter optimization\n",
    "4. **Negative Sample Generation** - Generate negative samples for contrastive learning\n",
    "5. **Contrastive Learning Training** - Train with contrastive loss + R-Drop\n",
    "6. **Inference & Evaluation** - Test and evaluate the model\n",
    "\n",
    "‚è∞ **Estimated Total Time**: 4-9 hours (depending on GPU)\n",
    "üîß **BARTpho Issue**: RESOLVED ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e46c94",
   "metadata": {},
   "source": [
    "## üöÄ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c159bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers==4.36.0 datasets==2.15.0 accelerate==0.25.0\n",
    "!pip install optuna==3.4.0 wandb==0.16.0 lightning==2.1.0\n",
    "!pip install sentencepiece tokenizers nltk sacrebleu evaluate rouge-score\n",
    "!pip install pandas numpy scikit-learn tqdm rich omegaconf hydra-core\n",
    "!pip install underthesea pyvi ipywidgets matplotlib seaborn\n",
    "\n",
    "# üîß BARTpho Fix Verification Test\n",
    "# Run this cell first to verify the fix is working\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üß™ Testing BARTpho tokenizer fix...\")\n",
    "\n",
    "# Create the fixed data_utils.py file directly in Colab\n",
    "data_utils_fixed = '''\"\"\"\n",
    "Data utilities for Vietnamese GEC with viGEC dataset - FIXED VERSION\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration, T5Tokenizer\n",
    "import logging\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "\n",
    "console = Console()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"Get model and tokenizer for Vietnamese GEC - FIXED VERSION\"\"\"\n",
    "    \n",
    "    console.print(f\"[bold blue]Loading model: {model_name}[/bold blue]\")\n",
    "    \n",
    "    if 'bartpho' in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        console.print(\"[green]‚úÖ BARTpho model loaded[/green]\")\n",
    "        \n",
    "    elif 'vit5' in model_name.lower():\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        \n",
    "        # Add task prefix for ViT5\n",
    "        if not hasattr(tokenizer, 'task_prefix'):\n",
    "            tokenizer.task_prefix = \"grammatical error correction: \"\n",
    "            console.print(f\"[yellow]Added ViT5 task prefix: {tokenizer.task_prefix}[/yellow]\")\n",
    "        \n",
    "        console.print(\"[green]‚úÖ ViT5 model loaded[/green]\")\n",
    "        \n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        console.print(\"[green]‚úÖ Generic model loaded[/green]\")\n",
    "    \n",
    "    # FIXED: Add special tokens if needed - Safe vocabulary checking\n",
    "    special_tokens = ['<gec>', '</gec>']\n",
    "    \n",
    "    try:\n",
    "        if hasattr(tokenizer, 'vocab'):\n",
    "            # Standard tokenizers (BERT, etc.)\n",
    "            vocab = tokenizer.vocab\n",
    "            console.print(\"[blue]Using .vocab attribute[/blue]\")\n",
    "        elif hasattr(tokenizer, 'get_vocab'):\n",
    "            # SentencePiece tokenizers (BARTpho, etc.)\n",
    "            vocab = tokenizer.get_vocab()\n",
    "            console.print(f\"[blue]Using .get_vocab() method - vocab size: {len(vocab)}[/blue]\")\n",
    "        else:\n",
    "            # Fallback: assume all tokens are new\n",
    "            vocab = {}\n",
    "            console.print(\"[yellow]No vocab access method found, skipping token check[/yellow]\")\n",
    "        \n",
    "        new_tokens = [token for token in special_tokens if token not in vocab]\n",
    "        \n",
    "        if new_tokens:\n",
    "            tokenizer.add_tokens(new_tokens)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "            console.print(f\"[yellow]Added {len(new_tokens)} new tokens: {new_tokens}[/yellow]\")\n",
    "        else:\n",
    "            console.print(\"[blue]No new tokens needed[/blue]\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        console.print(f\"[yellow]Warning: Could not check vocabulary - {e}[/yellow]\")\n",
    "        console.print(\"[yellow]Skipping special token addition[/yellow]\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def test_tokenizer_vocab_access(model_name: str):\n",
    "    \"\"\"Test vocabulary access for different tokenizer types\"\"\"\n",
    "    console.print(f\"[bold]Testing vocabulary access for: {model_name}[/bold]\")\n",
    "    \n",
    "    try:\n",
    "        model, tokenizer = get_model_and_tokenizer(model_name)\n",
    "        \n",
    "        # Test tokenization\n",
    "        test_text = \"T√¥i ƒëi h·ªçc tr∆∞·ªùng ƒë·∫°i h·ªçc.\"\n",
    "        if hasattr(tokenizer, 'task_prefix'):\n",
    "            test_text = tokenizer.task_prefix + test_text\n",
    "        \n",
    "        tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "        console.print(f\"[green]‚úÖ Tokenization successful - shape: {tokens['input_ids'].shape}[/green]\")\n",
    "        \n",
    "        # Test vocabulary access methods\n",
    "        if hasattr(tokenizer, 'vocab'):\n",
    "            console.print(f\"[green]‚úÖ Has .vocab attribute[/green]\")\n",
    "        elif hasattr(tokenizer, 'get_vocab'):\n",
    "            vocab = tokenizer.get_vocab()\n",
    "            console.print(f\"[green]‚úÖ Has .get_vocab() method - size: {len(vocab)}[/green]\")\n",
    "        else:\n",
    "            console.print(f\"[yellow]‚ö†Ô∏è No standard vocab access method[/yellow]\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå Error: {e}[/red]\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "'''\n",
    "\n",
    "# Write the fixed file\n",
    "with open('data_utils_fixed.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(data_utils_fixed)\n",
    "\n",
    "print(\"‚úÖ Created fixed data_utils.py\")\n",
    "\n",
    "# Test the fix\n",
    "try:\n",
    "    from data_utils_fixed import test_tokenizer_vocab_access, get_model_and_tokenizer\n",
    "    \n",
    "    print(\"\\nüîç Testing BARTpho tokenizer...\")\n",
    "    success_bartpho = test_tokenizer_vocab_access(\"vinai/bartpho-syllable\")\n",
    "    \n",
    "    if success_bartpho:\n",
    "        print(\"üéâ BARTpho tokenizer fix is working!\")\n",
    "    else:\n",
    "        print(\"‚ùå BARTpho test failed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ BARTpho Fix Status: ‚úÖ READY FOR USE\" if 'success_bartpho' in locals() and success_bartpho else \"‚ùå NEEDS ATTENTION\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (if needed)\n",
    "# !git clone https://github.com/your-repo/CL_GEC.git\n",
    "# %cd CL_GEC\n",
    "\n",
    "# Or upload files directly to Colab\n",
    "import os\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "os.makedirs('./evaluation_results', exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Directories created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f33240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload all Python files to Colab\n",
    "# Use the file upload button in Colab to upload:\n",
    "# - data_utils.py\n",
    "# - base_trainer.py  \n",
    "# - negative_sampler.py\n",
    "# - contrastive_trainer.py\n",
    "# - inference.py\n",
    "# - evaluator.py\n",
    "# - evaluate_model.py\n",
    "\n",
    "# Verify files are uploaded\n",
    "required_files = [\n",
    "    'data_utils.py', 'base_trainer.py', 'negative_sampler.py',\n",
    "    'contrastive_trainer.py', 'inference.py', 'evaluator.py', 'evaluate_model.py'\n",
    "]\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ {file} found\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} missing - please upload this file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e002faf",
   "metadata": {},
   "source": [
    "## üìä Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import torch\n",
    "import wandb\n",
    "from rich.console import Console\n",
    "from data_utils import load_vigec_dataset, save_processed_data, get_model_and_tokenizer\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "console.print(f\"üî• Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    console.print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    console.print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to wandb for experiment tracking\n",
    "!wandb login\n",
    "\n",
    "# Set wandb project\n",
    "wandb.login()\n",
    "console.print(\"üìà Wandb setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42649c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess viGEC dataset\n",
    "console.print(\"üì• Loading viGEC dataset...\")\n",
    "\n",
    "# Load the dataset\n",
    "data = load_vigec_dataset(dataset_name=\"phuhuy-se1/viGEC\")\n",
    "\n",
    "# Save processed data\n",
    "save_processed_data(data, \"./data/processed\")\n",
    "\n",
    "console.print(f\"‚úÖ Data preprocessing completed!\")\n",
    "for split, split_data in data.items():\n",
    "    console.print(f\"  {split}: {len(split_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76fffde",
   "metadata": {},
   "source": [
    "## üéØ Step 2: Base Model Training with Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your base model\n",
    "# Options: \"vinai/bartpho-syllable\", \"vinai/bartpho-word\", \"VietAI/vit5-base\", \"VietAI/vit5-large\"\n",
    "\n",
    "MODEL_NAME = \"vinai/bartpho-syllable\"  # Change this as needed\n",
    "\n",
    "console.print(f\"ü§ñ Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_trainer import BaseTrainer\n",
    "\n",
    "# Create base trainer\n",
    "base_trainer = BaseTrainer(\n",
    "    model_name=MODEL_NAME,\n",
    "    data_dir=\"./data/processed\",\n",
    "    output_dir=\"./models/base\",\n",
    "    hyperopt=True  # Set to False to skip hyperparameter optimization\n",
    ")\n",
    "\n",
    "console.print(\"üèóÔ∏è Base trainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60cd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start base model training\n",
    "# This will:\n",
    "# 1. Run hyperparameter optimization (30 trials)\n",
    "# 2. Train final model with best parameters\n",
    "# 3. Save model and tokenizer\n",
    "\n",
    "console.print(\"üöÄ Starting base model training...\")\n",
    "console.print(\"‚è∞ This may take 2-4 hours depending on your setup\")\n",
    "\n",
    "base_trainer.train()\n",
    "\n",
    "console.print(\"‚úÖ Base model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcdc01",
   "metadata": {},
   "source": [
    "## üé≠ Step 3: Negative Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from negative_sampler import NegativeSampleGenerator\n",
    "\n",
    "# Create negative sample generator using the trained base model\n",
    "BASE_MODEL_PATH = \"./models/base/final\"\n",
    "\n",
    "console.print(\"üé≠ Initializing negative sample generator...\")\n",
    "\n",
    "generator = NegativeSampleGenerator(\n",
    "    model_path=BASE_MODEL_PATH,\n",
    "    device=\"auto\"\n",
    ")\n",
    "\n",
    "console.print(\"‚úÖ Negative sample generator ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import load_processed_data\n",
    "import os\n",
    "\n",
    "# Load processed data\n",
    "data = load_processed_data(\"./data/processed\")\n",
    "\n",
    "# Generate contrastive datasets\n",
    "os.makedirs(\"./data/contrastive\", exist_ok=True)\n",
    "\n",
    "console.print(\"üîÑ Generating negative samples...\")\n",
    "console.print(\"‚è∞ This may take 1-2 hours depending on dataset size\")\n",
    "\n",
    "for split in ['train', 'validation']:\n",
    "    if split in data:\n",
    "        console.print(f\"Processing {split} split...\")\n",
    "        \n",
    "        output_path = f\"./data/contrastive/{split}_contrastive.json\"\n",
    "        \n",
    "        contrastive_data = generator.generate_contrastive_dataset(\n",
    "            data[split],\n",
    "            output_path,\n",
    "            batch_size=8,\n",
    "            max_samples=None  # Set to smaller number for testing, e.g., 1000\n",
    "        )\n",
    "        \n",
    "        # Analyze quality\n",
    "        generator.analyze_negatives_quality(contrastive_data, sample_size=5)\n",
    "\n",
    "console.print(\"‚úÖ Negative sample generation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6054d04",
   "metadata": {},
   "source": [
    "## üîÑ Step 4: Contrastive Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contrastive_trainer import ContrastiveTrainer\n",
    "\n",
    "# Create contrastive trainer\n",
    "contrastive_trainer = ContrastiveTrainer(\n",
    "    base_model_path=BASE_MODEL_PATH,\n",
    "    contrastive_data_dir=\"./data/contrastive\",\n",
    "    output_dir=\"./models/contrastive\",\n",
    "    hyperopt=True  # Set to False to skip hyperparameter optimization\n",
    ")\n",
    "\n",
    "console.print(\"üîÑ Contrastive trainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start contrastive learning training\n",
    "# This will:\n",
    "# 1. Run hyperparameter optimization for Œª, Œ≥, k\n",
    "# 2. Train final model with contrastive loss + R-Drop\n",
    "# 3. Save final contrastive model\n",
    "\n",
    "console.print(\"üöÄ Starting contrastive learning training...\")\n",
    "console.print(\"‚è∞ This may take 1-3 hours\")\n",
    "\n",
    "contrastive_trainer.train()\n",
    "\n",
    "console.print(\"‚úÖ Contrastive learning training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc86a8",
   "metadata": {},
   "source": [
    "## üîÆ Step 5: Inference with Contrastive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b70eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import GECInference\n",
    "\n",
    "# Load the trained contrastive model\n",
    "CONTRASTIVE_MODEL_PATH = \"./models/contrastive/final\"\n",
    "\n",
    "# Create inference engines\n",
    "console.print(\"üîÆ Initializing inference engines...\")\n",
    "\n",
    "# Contrastive search inference\n",
    "contrastive_inference = GECInference(\n",
    "    model_path=CONTRASTIVE_MODEL_PATH,\n",
    "    use_contrastive_search=True,\n",
    "    contrastive_alpha=0.7,\n",
    "    contrastive_k=5\n",
    ")\n",
    "\n",
    "# Beam search inference for comparison\n",
    "beam_inference = GECInference(\n",
    "    model_path=CONTRASTIVE_MODEL_PATH,\n",
    "    use_contrastive_search=False\n",
    ")\n",
    "\n",
    "console.print(\"‚úÖ Inference engines ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f028e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with sample texts\n",
    "test_texts = [\n",
    "    \"T√¥i ƒëi h·ªçc tr∆∞·ªùng ƒë·∫°i h·ªçc.\",\n",
    "    \"H√¥m nay t√¥i kh√¥ng ƒëi l√†m.\",\n",
    "    \"C√¥ ·∫•y r·∫•t ƒë·∫πp v√† th√¥ng minh.\",\n",
    "    \"Ch√∫ng t√¥i s·∫Ω ƒëi du l·ªãch v√†o tu·∫ßn t·ªõi.\",\n",
    "    \"Anh ·∫•y l√†m vi·ªác ·ªü c√¥ng ty l·ªõn.\"\n",
    "]\n",
    "\n",
    "console.print(\"üß™ Testing inference on sample texts...\")\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    console.print(f\"\\n[bold cyan]Example {i+1}:[/bold cyan]\")\n",
    "    console.print(f\"[yellow]Original:[/yellow] {text}\")\n",
    "    \n",
    "    # Contrastive search\n",
    "    contrastive_result = contrastive_inference.correct_text(text)\n",
    "    console.print(f\"[green]Contrastive:[/green] {contrastive_result}\")\n",
    "    \n",
    "    # Beam search\n",
    "    beam_result = beam_inference.correct_text(text)\n",
    "    console.print(f\"[blue]Beam:[/blue] {beam_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive correction (optional)\n",
    "# Uncomment to enable interactive mode\n",
    "\n",
    "# console.print(\"üéÆ Interactive mode - Enter text to correct (type 'quit' to exit):\")\n",
    "# contrastive_inference.interactive_correction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f1867",
   "metadata": {},
   "source": [
    "## üìä Step 6: Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d018b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_model import ModelEvaluator\n",
    "\n",
    "# Create model evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    model_path=CONTRASTIVE_MODEL_PATH,\n",
    "    data_dir=\"./data/processed\",\n",
    "    output_dir=\"./evaluation_results\"\n",
    ")\n",
    "\n",
    "console.print(\"üìä Model evaluator initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "console.print(\"üîç Starting comprehensive evaluation...\")\n",
    "console.print(\"‚è∞ This may take 30-60 minutes\")\n",
    "\n",
    "# Evaluate on test set with different decoding strategies\n",
    "evaluation_results = evaluator.evaluate_on_test_set(\n",
    "    max_samples=None,  # Set to smaller number for testing, e.g., 500\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "console.print(\"‚úÖ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error type analysis\n",
    "console.print(\"üî¨ Running error type analysis...\")\n",
    "\n",
    "error_analysis = evaluator.evaluate_error_types(\n",
    "    max_samples=1000  # Limit for faster analysis\n",
    ")\n",
    "\n",
    "console.print(\"‚úÖ Error type analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fabe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation visualizations\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Show evaluation comparison plot\n",
    "plot_path = \"./evaluation_results/evaluation_comparison.png\"\n",
    "if os.path.exists(plot_path):\n",
    "    console.print(\"üìà Evaluation Comparison Visualization:\")\n",
    "    display(Image(plot_path))\n",
    "else:\n",
    "    console.print(\"‚ùå Visualization not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show evaluation results summary\n",
    "import pandas as pd\n",
    "\n",
    "# Load and display comparison table\n",
    "csv_path = \"./evaluation_results/strategy_comparison.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    console.print(\"üìã Strategy Comparison Results:\")\n",
    "    display(df)\n",
    "else:\n",
    "    console.print(\"‚ùå Comparison table not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797276b8",
   "metadata": {},
   "source": [
    "## üìÅ Results and Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432def41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of trained models and results\n",
    "console.print(\"\\n[bold green]üéâ Training Pipeline Completed Successfully![/bold green]\")\n",
    "\n",
    "console.print(\"\\nüìÅ [bold]Generated Models and Results:[/bold]\")\n",
    "console.print(f\"  üì¶ Base Model: ./models/base/final\")\n",
    "console.print(f\"  üîÑ Contrastive Model: ./models/contrastive/final\")\n",
    "console.print(f\"  üìä Evaluation Results: ./evaluation_results/\")\n",
    "console.print(f\"  üé≠ Contrastive Data: ./data/contrastive/\")\n",
    "\n",
    "# List all generated files\n",
    "import os\n",
    "\n",
    "def list_files_recursive(directory):\n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            files.append(os.path.join(root, filename))\n",
    "    return files\n",
    "\n",
    "console.print(\"\\nüìã [bold]All Generated Files:[/bold]\")\n",
    "\n",
    "for directory in ['./models', './evaluation_results', './data/contrastive']:\n",
    "    if os.path.exists(directory):\n",
    "        files = list_files_recursive(directory)\n",
    "        console.print(f\"\\n  üìÇ {directory}:\")\n",
    "        for file in files[:10]:  # Show first 10 files\n",
    "            console.print(f\"    üìÑ {file}\")\n",
    "        if len(files) > 10:\n",
    "            console.print(f\"    ... and {len(files) - 10} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db072f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models and results (for local use)\n",
    "# Uncomment to create zip files for download\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# console.print(\"üì¶ Creating downloadable archives...\")\n",
    "\n",
    "# # Create zip files\n",
    "# shutil.make_archive('contrastive_gec_model', 'zip', './models/contrastive/final')\n",
    "# shutil.make_archive('evaluation_results', 'zip', './evaluation_results')\n",
    "\n",
    "# console.print(\"‚úÖ Archives created:\")\n",
    "# console.print(\"  üì¶ contrastive_gec_model.zip - Trained model\")\n",
    "# console.print(\"  üì¶ evaluation_results.zip - Evaluation results\")\n",
    "# console.print(\"\\nüíæ Use the file browser to download these files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab3d72",
   "metadata": {},
   "source": [
    "## üöÄ Quick Usage Guide\n",
    "\n",
    "Once training is complete, you can use the model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick usage example\n",
    "console.print(\"üöÄ [bold]Quick Usage Example:[/bold]\")\n",
    "\n",
    "# Load the model\n",
    "from inference import GECInference\n",
    "\n",
    "# Initialize\n",
    "gec_model = GECInference(\n",
    "    model_path=\"./models/contrastive/final\",\n",
    "    use_contrastive_search=True\n",
    ")\n",
    "\n",
    "# Correct text\n",
    "text = \"T√¥i ƒëi h·ªçc tr∆∞·ªùng ƒë·∫°i h·ªçc.\"\n",
    "corrected = gec_model.correct_text(text)\n",
    "\n",
    "console.print(f\"Original: {text}\")\n",
    "console.print(f\"Corrected: {corrected}\")\n",
    "\n",
    "console.print(\"\\nüí° [bold]Usage Tips:[/bold]\")\n",
    "console.print(\"  üéØ Use contrastive_search=True for better quality\")\n",
    "console.print(\"  ‚ö° Use contrastive_search=False for faster inference\")\n",
    "console.print(\"  üìä Adjust alpha and k parameters for fine-tuning\")\n",
    "console.print(\"  üìÅ Process files with correct_file() method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d2fae",
   "metadata": {},
   "source": [
    "## üìù Configuration and Hyperparameters\n",
    "\n",
    "Key hyperparameters used in this pipeline:\n",
    "\n",
    "### Base Training:\n",
    "- **Learning Rate**: Optimized via Optuna (typically 1e-5 to 1e-4)\n",
    "- **Label Smoothing**: 0.1\n",
    "- **Batch Size**: 8-32 (depending on GPU memory)\n",
    "- **Max Length**: 384 tokens\n",
    "- **Epochs**: 5-10\n",
    "\n",
    "### Contrastive Learning:\n",
    "- **Œª (lambda_cl)**: 1.0 (balance between CE and CL loss)\n",
    "- **Œ≥ (temperature)**: 0.25 (contrastive loss temperature)\n",
    "- **R-Drop Œ±**: 4.0 (R-Drop regularization strength)\n",
    "- **Epochs**: 3-5\n",
    "\n",
    "### Contrastive Search:\n",
    "- **Œ± (alpha)**: 0.7 (balance between confidence and diversity)\n",
    "- **k**: 5 (top-k candidates)\n",
    "- **Beam Size**: 1 (as recommended in paper)\n",
    "\n",
    "These parameters can be adjusted based on your specific needs and computational resources."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
