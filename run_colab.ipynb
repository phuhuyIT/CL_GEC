{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627bd662",
   "metadata": {},
   "source": [
    "# üáªüá≥ Vietnamese GEC with Contrastive Learning - Google Colab\n",
    "\n",
    "**Clean & Simple**: Clone repository and run training pipeline for Vietnamese Grammatical Error Correction with BARTpho/ViT5 + Contrastive Learning.\n",
    "\n",
    "## üìã Pipeline Overview:\n",
    "1. **Setup & Clone Repository** - Install dependencies and clone source code\n",
    "2. **Data Preparation** - Load and preprocess viGEC dataset  \n",
    "3. **Base Model Training** - Fine-tune BARTpho/ViT5 with hyperparameter optimization\n",
    "4. **Negative Sample Generation** - Generate negative samples for contrastive learning\n",
    "5. **Contrastive Learning Training** - Train with contrastive loss + R-Drop\n",
    "6. **Inference & Evaluation** - Test and evaluate the model\n",
    "\n",
    "‚è∞ **Estimated Total Time**: 4-9 hours (depending on GPU)  \n",
    "üöÄ **Ready to Run**: All import issues fixed, clean codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e33c59",
   "metadata": {},
   "source": [
    "## üöÄ Step 1: Setup and Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f110603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"üî• CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available - training will be very slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5032aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install numpy\n",
    "!pip3 install torch torchaudio torchvision torchtext torchdata\n",
    "!pip install transformers datasets accelerate\n",
    "!pip install optuna  wandb lightning\n",
    "!pip install sentencepiece tokenizers nltk sacrebleu evaluate rouge-score\n",
    "!pip install pandas scikit-learn tqdm rich omegaconf hydra-core\n",
    "!pip install underthesea pyvi ipywidgets matplotlib seaborn\n",
    "!pip install -U datasets huggingface_hub fsspec\n",
    "!pip install optuna-integration[pytorch_lightning]\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac40a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your actual GitHub repository URL)\n",
    "import os\n",
    "\n",
    "# Change this to your actual repository URL\n",
    "REPO_URL = \"https://github.com/YOUR_USERNAME/CL_GEC.git\"  # Update this!\n",
    "PROJECT_DIR = \"/content/CL_GEC\"\n",
    "\n",
    "# Clone or update repository\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print(f\"üì• Cloning repository from {REPO_URL}...\")\n",
    "    !git clone {REPO_URL} {PROJECT_DIR}\n",
    "else:\n",
    "    print(\"üìÅ Repository already exists, pulling latest changes...\")\n",
    "    %cd {PROJECT_DIR}\n",
    "    !git pull\n",
    "\n",
    "# Change to project directory\n",
    "%cd {PROJECT_DIR}\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# List files to verify\n",
    "print(\"\\nüìã Project files:\")\n",
    "!ls -la *.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b671a",
   "metadata": {},
   "source": [
    "## üìä Step 2: Data Preparation and System Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11ce7b",
   "metadata": {},
   "source": [
    "## üîß New Features & Parameters\n",
    "\n",
    "### ‚ú® Enhanced BaseTrainer Features:\n",
    "\n",
    "1. **üìä Dataset Configuration**:\n",
    "   - `dataset_name`: Choose dataset version (e.g., \"phuhuy-se1/viGEC-v2\")\n",
    "   - `train_subset_ratio`: Use subset of training data (0.0-1.0) \n",
    "   - `validation_subset_ratio`: Use subset of validation data\n",
    "   - `test_subset_ratio`: Use subset of test data\n",
    "\n",
    "2. **üîç Customizable Search Space**:\n",
    "   - Define learning rate ranges\n",
    "   - Configure weight decay options\n",
    "   - Set batch size choices\n",
    "   - Customize warmup ratios\n",
    "\n",
    "3. **‚ö° Flexible Training Modes**:\n",
    "   - Hyperparameter optimization only\n",
    "   - Training with specific parameters\n",
    "   - Combined optimization + training\n",
    "\n",
    "### üí° Benefits:\n",
    "- **Faster experimentation** with data subsets\n",
    "- **Better hyperparameter control** \n",
    "- **Dataset version management**\n",
    "- **Memory-efficient training** for limited resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c36e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports and system readiness\n",
    "from data_utils import (\n",
    "    load_vigec_dataset, \n",
    "    get_model_and_tokenizer, \n",
    "    can_train_base_model,\n",
    "    check_dataset_format\n",
    ")\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Check system readiness\n",
    "console.print(\"[bold blue]üîç System Readiness Check[/bold blue]\")\n",
    "system_ready = can_train_base_model()\n",
    "\n",
    "# Check dataset format\n",
    "console.print(\"\\n[bold blue]üìã Dataset Format Check[/bold blue]\")\n",
    "dataset_ready = check_dataset_format()\n",
    "\n",
    "if system_ready and dataset_ready:\n",
    "    console.print(\"\\n[bold green]‚úÖ All checks passed! Ready to proceed.[/bold green]\")\n",
    "else:\n",
    "    console.print(\"\\n[bold red]‚ùå System not ready. Please check requirements.[/bold red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa031742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset with configurable parameters\n",
    "console.print(\"[bold blue]üìä Loading viGEC Dataset[/bold blue]\")\n",
    "\n",
    "# Dataset configuration - modify these as needed\n",
    "DATASET_CONFIG = {\n",
    "    \"dataset_name\": \"phuhuy-se1/viGEC\",  # Change to \"phuhuy-se1/viGEC-v2\" for version 2\n",
    "    \"train_subset_ratio\": 0.1,  # Use 10% of training data for faster processing in Colab\n",
    "    \"validation_subset_ratio\": 0.2,  # Use 20% of validation data  \n",
    "    \"test_subset_ratio\": 0.05   # Use 5% of test data for faster evaluation\n",
    "}\n",
    "\n",
    "console.print(f\"[yellow]üìã Dataset Configuration:[/yellow]\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    console.print(f\"  {key}: {value}\")\n",
    "\n",
    "# Load dataset with configurable parameters\n",
    "data = load_vigec_dataset(\n",
    "    dataset_name=DATASET_CONFIG[\"dataset_name\"],\n",
    "    train_subset_ratio=DATASET_CONFIG[\"train_subset_ratio\"],\n",
    "    validation_subset_ratio=DATASET_CONFIG[\"validation_subset_ratio\"],\n",
    "    test_subset_ratio=DATASET_CONFIG[\"test_subset_ratio\"]\n",
    ")\n",
    "\n",
    "console.print(f\"\\n[green]Dataset loaded successfully![/green]\")\n",
    "for split, split_data in data.items():\n",
    "    console.print(f\"  {split}: {len(split_data)} samples\")\n",
    "    \n",
    "# Show subset ratios effect\n",
    "console.print(f\"\\n[blue]üìä Subset Effects:[/blue]\")\n",
    "console.print(f\"  Training samples: ~{len(data['train'])} (subset ratio: {DATASET_CONFIG['train_subset_ratio']})\")\n",
    "console.print(f\"  Validation samples: ~{len(data['validation'])} (subset ratio: {DATASET_CONFIG['validation_subset_ratio']})\")\n",
    "console.print(f\"  Test samples: ~{len(data['test'])} (subset ratio: {DATASET_CONFIG['test_subset_ratio']})\")\n",
    "\n",
    "# Save processed data\n",
    "from data_utils import save_processed_data\n",
    "save_processed_data(data, \"./data/processed\")\n",
    "console.print(\"\\n[blue]‚úÖ Data saved to ./data/processed/[/blue]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201dc15d",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: Model Selection and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cd00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your model - uncomment one of these:\n",
    "MODEL_NAME = \"vinai/bartpho-syllable\"  # Recommended for Vietnamese\n",
    "# MODEL_NAME = \"VietAI/vit5-base\"     # Alternative option\n",
    "# MODEL_NAME = \"VietAI/vit5-large\"    # Larger model (requires more GPU memory)\n",
    "\n",
    "console.print(f\"[bold blue]ü§ñ Loading Model: {MODEL_NAME}[/bold blue]\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = get_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "console.print(f\"[green]‚úÖ Model loaded successfully![/green]\")\n",
    "console.print(f\"  Model: {model.__class__.__name__}\")\n",
    "console.print(f\"  Tokenizer: {tokenizer.__class__.__name__}\")\n",
    "console.print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"T√¥i ƒëang h·ªçc ti·∫øng vi·ªát.\"\n",
    "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "console.print(f\"\\n[blue]üß™ Tokenization Test:[/blue]\")\n",
    "console.print(f\"  Input: {test_text}\")\n",
    "console.print(f\"  Tokens: {tokens['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39334726",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 4: Base Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bb88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "TRAINING_CONFIG = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"output_dir\": \"./models/base_model\",\n",
    "    \"max_epochs\": 3,  # Reduced for Colab\n",
    "    \"batch_size\": 8,  # Adjust based on GPU memory\n",
    "    \"use_wandb\": True,  # Set to False if you don't want to use Weights & Biases\n",
    "    \"run_optimization\": False,  # Set to True for hyperparameter optimization (takes longer)\n",
    "    \n",
    "    # New dataset parameters\n",
    "    \"dataset_name\": \"phuhuy-se1/viGEC\",  # Change to \"phuhuy-se1/viGEC-v2\" for version 2\n",
    "    \"train_subset_ratio\": 0.1,  # Use 10% of training data for faster training in Colab\n",
    "    \"validation_subset_ratio\": 0.2,  # Use 20% of validation data\n",
    "    \"test_subset_ratio\": 0.05,  # Use 5% of test data\n",
    "    \n",
    "    # Custom search space for hyperparameter optimization (if enabled)\n",
    "    \"search_space\": {\n",
    "        'learning_rate': {'low': 1e-5, 'high': 5e-4, 'log': True},\n",
    "        'weight_decay': {'low': 0.001, 'high': 0.05, 'log': True},\n",
    "        'label_smoothing': {'low': 0.0, 'high': 0.2},\n",
    "        'batch_size': [8, 16, 24],  # Smaller batch sizes for Colab\n",
    "        'warmup_ratio': {'low': 0.05, 'high': 0.15}\n",
    "    }\n",
    "}\n",
    "\n",
    "console.print(\"[bold blue]üèãÔ∏è Base Model Training Configuration:[/bold blue]\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    if key != \"search_space\":  # Don't print the search space dict for brevity\n",
    "        console.print(f\"  {key}: {value}\")\n",
    "\n",
    "if TRAINING_CONFIG[\"run_optimization\"]:\n",
    "    console.print(\"\\n[yellow]‚ö†Ô∏è Hyperparameter optimization enabled - this will take longer but may improve results[/yellow]\")\n",
    "    console.print(f\"[blue]Search space configured with {len(TRAINING_CONFIG['search_space'])} parameters[/blue]\")\n",
    "else:\n",
    "    console.print(\"\\n[blue]‚ÑπÔ∏è Using default parameters for faster training[/blue]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2820213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start base model training\n",
    "from base_trainer import BaseTrainer\n",
    "\n",
    "console.print(\"[bold green]üöÄ Starting Base Model Training...[/bold green]\")\n",
    "\n",
    "# Create base trainer with enhanced parameters\n",
    "base_trainer = BaseTrainer(\n",
    "    model_name=TRAINING_CONFIG[\"model_name\"],\n",
    "    data_dir=\"./data/processed\",  # Use the processed data directory\n",
    "    output_dir=TRAINING_CONFIG[\"output_dir\"],\n",
    "    hyperopt=TRAINING_CONFIG[\"run_optimization\"],  # Enable/disable hyperopt\n",
    "    use_wandb=TRAINING_CONFIG[\"use_wandb\"],\n",
    "    \n",
    "    # New dataset parameters\n",
    "    dataset_name=TRAINING_CONFIG[\"dataset_name\"],\n",
    "    train_subset_ratio=TRAINING_CONFIG[\"train_subset_ratio\"],\n",
    "    validation_subset_ratio=TRAINING_CONFIG[\"validation_subset_ratio\"],\n",
    "    test_subset_ratio=TRAINING_CONFIG[\"test_subset_ratio\"]\n",
    ")\n",
    "\n",
    "# Train the model with the correct method signature\n",
    "if TRAINING_CONFIG[\"run_optimization\"]:\n",
    "    console.print(\"[yellow]üîç Running hyperparameter optimization...[/yellow]\")\n",
    "    study = base_trainer.optimize_hyperparameters(\n",
    "        n_trials=5,  # Reduced for Colab (increase to 10-20 for better results)\n",
    "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "        search_space=TRAINING_CONFIG[\"search_space\"]\n",
    "    )\n",
    "    console.print(f\"[green]‚úÖ Best parameters: {study.best_params}[/green]\")\n",
    "    console.print(f\"[green]‚úÖ Best F0.5 score: {study.best_value:.4f}[/green]\")\n",
    "    \n",
    "    # Train final model with best parameters  \n",
    "    console.print(\"[blue]üèÉ Training final model with best parameters...[/blue]\")\n",
    "    trained_model = base_trainer.train_with_params(\n",
    "        params=study.best_params,\n",
    "        max_epochs=TRAINING_CONFIG[\"max_epochs\"],\n",
    "        batch_size=study.best_params.get('batch_size', TRAINING_CONFIG[\"batch_size\"])\n",
    "    )\n",
    "else:\n",
    "    console.print(\"[blue]üèÉ Training with default parameters...[/blue]\")\n",
    "    \n",
    "    # Train the model (hyperopt is controlled by the hyperopt parameter in constructor)\n",
    "    trained_model = base_trainer.train(\n",
    "        max_epochs=TRAINING_CONFIG[\"max_epochs\"],\n",
    "        batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "        search_space=None  # No search space needed for default training\n",
    "    )\n",
    "\n",
    "console.print(\"[bold green]‚úÖ Base model training completed![/bold green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d4aaa",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Negative Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70bb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative samples for contrastive learning\n",
    "from negative_sampler import NegativeSampler\n",
    "import os\n",
    "\n",
    "console.print(\"[bold blue]üéØ Generating Negative Samples...[/bold blue]\")\n",
    "\n",
    "# Create negative sampler (use the final model from training)\n",
    "base_model_path = os.path.join(TRAINING_CONFIG[\"output_dir\"], \"final_model\")\n",
    "\n",
    "# Check if trained model exists\n",
    "if os.path.exists(base_model_path):\n",
    "    console.print(f\"[green]‚úÖ Using trained model from {base_model_path}[/green]\")\n",
    "    model_path = base_model_path\n",
    "else:\n",
    "    console.print(f\"[yellow]‚ö†Ô∏è Trained model not found, using base model {MODEL_NAME}[/yellow]\")\n",
    "    model_path = MODEL_NAME\n",
    "\n",
    "negative_sampler = NegativeSampler(\n",
    "    model_path=model_path,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "# Generate negative samples for training data\n",
    "# Use smaller subset for Colab to avoid memory issues\n",
    "train_subset = data['train'][:1000] if len(data['train']) > 1000 else data['train']\n",
    "\n",
    "contrastive_data = negative_sampler.generate_contrastive_dataset(\n",
    "    data=train_subset,\n",
    "    num_negatives=3,  # Generate 3 negative samples per positive\n",
    "    output_file=\"./data/contrastive_train.json\"\n",
    ")\n",
    "\n",
    "console.print(f\"[green]‚úÖ Generated {len(contrastive_data)} contrastive samples![/green]\")\n",
    "console.print(\"[blue]üíæ Saved to ./data/contrastive_train.json[/blue]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622975a9",
   "metadata": {},
   "source": [
    "## üî• Step 6: Contrastive Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f588ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive learning training\n",
    "from contrastive_trainer import ContrastiveTrainer\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "console.print(\"[bold blue]üî• Starting Contrastive Learning Training...[/bold blue]\")\n",
    "\n",
    "# First, we need to prepare the contrastive data in the expected format\n",
    "contrastive_data_dir = \"./data/contrastive\"\n",
    "os.makedirs(contrastive_data_dir, exist_ok=True)\n",
    "\n",
    "# Convert the contrastive data to the expected format for validation\n",
    "validation_contrastive = []\n",
    "for item in data['validation'][:200]:  # Use subset for validation\n",
    "    validation_contrastive.append({\n",
    "        'source': item['source'],\n",
    "        'positive': item['target'],\n",
    "        'negatives': [item['source']]  # Simple negative sample\n",
    "    })\n",
    "\n",
    "# Save validation data\n",
    "with open(os.path.join(contrastive_data_dir, \"validation_contrastive.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(validation_contrastive, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Copy training contrastive data to the expected location\n",
    "if os.path.exists(\"./data/contrastive_train.json\"):\n",
    "    shutil.copy(\"./data/contrastive_train.json\", \n",
    "                os.path.join(contrastive_data_dir, \"train_contrastive.json\"))\n",
    "\n",
    "# Create contrastive trainer\n",
    "contrastive_trainer = ContrastiveTrainer(\n",
    "    base_model_path=os.path.join(TRAINING_CONFIG[\"output_dir\"], \"final_model\"),\n",
    "    contrastive_data_dir=contrastive_data_dir,\n",
    "    output_dir=\"./models/contrastive_model\",\n",
    "    hyperopt=False  # Disable hyperopt for faster training in Colab\n",
    ")\n",
    "\n",
    "# Train with contrastive learning\n",
    "contrastive_trainer.train()\n",
    "\n",
    "console.print(\"[bold green]‚úÖ Contrastive learning training completed![/bold green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f940b5f",
   "metadata": {},
   "source": [
    "## üß™ Step 7: Inference and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539896fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for inference\n",
    "from inference import GECInference\n",
    "import os\n",
    "\n",
    "console.print(\"[bold blue]üß™ Setting up Inference...[/bold blue]\")\n",
    "\n",
    "# Determine which model to use for inference\n",
    "contrastive_model_path = \"./models/contrastive_model\"\n",
    "base_model_path = TRAINING_CONFIG[\"output_dir\"]\n",
    "\n",
    "if os.path.exists(contrastive_model_path) and os.listdir(contrastive_model_path):\n",
    "    model_path = contrastive_model_path\n",
    "    console.print(f\"[green]‚úÖ Using contrastive model from {model_path}[/green]\")\n",
    "elif os.path.exists(base_model_path) and os.listdir(base_model_path):\n",
    "    model_path = base_model_path\n",
    "    console.print(f\"[yellow]‚ö†Ô∏è Using base model from {model_path}[/yellow]\")\n",
    "else:\n",
    "    model_path = MODEL_NAME\n",
    "    console.print(f\"[blue]‚ÑπÔ∏è Using original model {model_path}[/blue]\")\n",
    "\n",
    "# Create inference engine with the best available model\n",
    "gec_inference = GECInference(\n",
    "    model_path=model_path,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "console.print(\"[green]‚úÖ Inference engine ready![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing\n",
    "console.print(\"[bold blue]üéÆ Interactive Testing[/bold blue]\")\n",
    "\n",
    "# Test samples\n",
    "test_sentences = [\n",
    "    \"T√¥i ƒëang h·ªçc ti·∫øng vi·ªát ·ªü tr∆∞·ªùng ƒë·∫°i h·ªçc.\",\n",
    "    \"H√¥m nay tr·ªùi r·∫•t ƒë·∫πp v√† t√¥i mu·ªën ƒëi ch∆°i.\",\n",
    "    \"C√¥ ·∫•y l√†m vi·ªác t·∫°i m·ªôt c√¥ng ty l·ªõn ·ªü H√† N·ªôi.\",\n",
    "    \"Ch√∫ng t√¥i s·∫Ω ƒëi du l·ªãch v√†o cu·ªëi tu·∫ßn n√†y.\"\n",
    "]\n",
    "\n",
    "console.print(\"\\n[yellow]üìù Test Results:[/yellow]\")\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    corrected = gec_inference.correct_text(sentence)\n",
    "    console.print(f\"\\n{i}. Original: {sentence}\")\n",
    "    console.print(f\"   Corrected: {corrected}\")\n",
    "\n",
    "# Custom input\n",
    "console.print(\"\\n[bold cyan]‚úèÔ∏è Try your own text:[/bold cyan]\")\n",
    "print(\"Enter Vietnamese text to correct (or 'quit' to exit):\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"> \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    corrected = gec_inference.correct_text(user_input)\n",
    "    print(f\"Corrected: {corrected}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "from evaluator import F05Evaluator\n",
    "\n",
    "console.print(\"[bold blue]üìä Evaluating on Test Set...[/bold blue]\")\n",
    "\n",
    "# Create evaluator - check if we need to pass tokenizer\n",
    "try:\n",
    "    # Try with tokenizer first\n",
    "    evaluator = F05Evaluator(tokenizer=gec_inference.tokenizer)\n",
    "except:\n",
    "    # Fallback to no tokenizer\n",
    "    evaluator = F05Evaluator()\n",
    "\n",
    "# Evaluate on test set (using subset for faster evaluation)\n",
    "test_data_subset = data['test'][:100]  # Use 100 samples for evaluation\n",
    "sources = [item['source'] for item in test_data_subset]\n",
    "references = [item['target'] for item in test_data_subset]\n",
    "\n",
    "# Generate predictions\n",
    "console.print(\"[yellow]üîÆ Generating predictions...[/yellow]\")\n",
    "predictions = []\n",
    "for i, source in enumerate(sources):\n",
    "    if i % 20 == 0:  # Progress indicator\n",
    "        console.print(f\"[blue]Processing {i+1}/{len(sources)}...[/blue]\")\n",
    "    pred = gec_inference.correct_text(source)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Calculate metrics\n",
    "console.print(\"[yellow]üìà Calculating metrics...[/yellow]\")\n",
    "try:\n",
    "    # Try batch evaluation first\n",
    "    results = evaluator.evaluate_batch(predictions, references, sources)\n",
    "except AttributeError:\n",
    "    # Fallback to individual evaluation\n",
    "    f05_scores = []\n",
    "    for pred, ref, src in zip(predictions, references, sources):\n",
    "        f05 = evaluator.calculate_f05(src, pred, ref)\n",
    "        f05_scores.append(f05)\n",
    "    \n",
    "    results = {\n",
    "        \"f05_score\": np.mean(f05_scores),\n",
    "        \"num_samples\": len(f05_scores)\n",
    "    }\n",
    "\n",
    "console.print(\"\\n[bold green]üìà Evaluation Results:[/bold green]\")\n",
    "for metric, value in results.items():\n",
    "    if isinstance(value, float):\n",
    "        console.print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        console.print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Show some examples\n",
    "console.print(\"\\n[bold cyan]üîç Sample Results:[/bold cyan]\")\n",
    "for i in range(min(5, len(sources))):\n",
    "    console.print(f\"\\n{i+1}. Source: {sources[i]}\")\n",
    "    console.print(f\"   Target: {references[i]}\")\n",
    "    console.print(f\"   Prediction: {predictions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b2389",
   "metadata": {},
   "source": [
    "## üíæ Step 8: Save and Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db60851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and create export package\n",
    "import json\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "console.print(\"[bold blue]üíæ Saving Results and Creating Export Package...[/bold blue]\")\n",
    "\n",
    "# Create results summary\n",
    "results_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"training_config\": TRAINING_CONFIG,\n",
    "    \"evaluation_results\": results,\n",
    "    \"test_samples\": len(test_data),\n",
    "    \"model_paths\": {\n",
    "        \"base_model\": TRAINING_CONFIG[\"output_dir\"],\n",
    "        \"contrastive_model\": \"./models/contrastive_model\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(\"./results_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "console.print(\"[green]‚úÖ Results saved to ./results_summary.json[/green]\")\n",
    "\n",
    "# Create downloadable package\n",
    "console.print(\"[yellow]üì¶ Creating export package...[/yellow]\")\n",
    "\n",
    "with zipfile.ZipFile(\"vietnamese_gec_models.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add results\n",
    "    zipf.write(\"results_summary.json\")\n",
    "    \n",
    "    # Add model files (if they exist)\n",
    "    import glob\n",
    "    for model_file in glob.glob(\"./models/**/*.bin\", recursive=True):\n",
    "        zipf.write(model_file)\n",
    "    for config_file in glob.glob(\"./models/**/config.json\", recursive=True):\n",
    "        zipf.write(config_file)\n",
    "    \n",
    "    # Add data samples\n",
    "    if os.path.exists(\"./data/contrastive_train.json\"):\n",
    "        zipf.write(\"./data/contrastive_train.json\")\n",
    "\n",
    "console.print(\"[bold green]üéâ Export package created: vietnamese_gec_models.zip[/bold green]\")\n",
    "console.print(\"[blue]üìÅ You can download this file from the Colab file browser[/blue]\")\n",
    "\n",
    "# Display final summary\n",
    "console.print(\"\\n[bold cyan]üèÜ Training Pipeline Completed Successfully![/bold cyan]\")\n",
    "console.print(f\"[green]‚úÖ Base model trained and saved[/green]\")\n",
    "console.print(f\"[green]‚úÖ Contrastive learning applied[/green]\")\n",
    "console.print(f\"[green]‚úÖ Model evaluated on test set[/green]\")\n",
    "console.print(f\"[green]‚úÖ Results exported for download[/green]\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
