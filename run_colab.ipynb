{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627bd662",
   "metadata": {},
   "source": [
    "# üáªüá≥ Vietnamese GEC with Contrastive Learning - Google Colab\n",
    "\n",
    "**Clean & Simple**: Clone repository and run training pipeline for Vietnamese Grammatical Error Correction with BARTpho/ViT5 + Contrastive Learning.\n",
    "\n",
    "## üìã Pipeline Overview:\n",
    "1. **Setup & Clone Repository** - Install dependencies and clone source code\n",
    "2. **Data Preparation** - Load and preprocess viGEC dataset  \n",
    "3. **Base Model Training** - Fine-tune BARTpho/ViT5 with hyperparameter optimization\n",
    "4. **Negative Sample Generation** - Generate negative samples for contrastive learning\n",
    "5. **Contrastive Learning Training** - Train with contrastive loss + R-Drop\n",
    "6. **Inference & Evaluation** - Test and evaluate the model\n",
    "\n",
    "‚è∞ **Estimated Total Time**: 4-9 hours (depending on GPU)  \n",
    "üöÄ **Ready to Run**: All import issues fixed, clean codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e33c59",
   "metadata": {},
   "source": [
    "## üöÄ Step 1: Setup and Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f110603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"üî• CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available - training will be very slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5032aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages including comprehensive metrics\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install numpy\n",
    "!pip3 install torch torchaudio torchvision torchtext torchdata\n",
    "!pip install transformers datasets accelerate\n",
    "!pip install optuna  wandb lightning\n",
    "!pip install sentencepiece tokenizers nltk sacrebleu evaluate rouge-score\n",
    "!pip install pandas scikit-learn tqdm rich omegaconf hydra-core\n",
    "!pip install underthesea pyvi ipywidgets matplotlib seaborn\n",
    "!pip install -U datasets huggingface_hub fsspec\n",
    "!pip install optuna-integration[pytorch_lightning]\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "print(\"üéØ Comprehensive metrics available:\")\n",
    "print(\"   ‚Ä¢ F0.5, Precision, Recall (Edit-level)\")\n",
    "print(\"   ‚Ä¢ BLEU, GLEU (Translation metrics)\")\n",
    "print(\"   ‚Ä¢ ROUGE-1, ROUGE-2, ROUGE-L (Token overlap)\")\n",
    "print(\"   ‚Ä¢ Input-preserving Edit Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac40a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your actual GitHub repository URL)\n",
    "import os\n",
    "\n",
    "# Change this to your actual repository URL\n",
    "REPO_URL = \"https://github.com/YOUR_USERNAME/CL_GEC.git\"  # Update this!\n",
    "PROJECT_DIR = \"/content/CL_GEC\"\n",
    "\n",
    "# Clone or update repository\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print(f\"üì• Cloning repository from {REPO_URL}...\")\n",
    "    !git clone {REPO_URL} {PROJECT_DIR}\n",
    "else:\n",
    "    print(\"üìÅ Repository already exists, pulling latest changes...\")\n",
    "    %cd {PROJECT_DIR}\n",
    "    !git pull\n",
    "\n",
    "# Change to project directory\n",
    "%cd {PROJECT_DIR}\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# List files to verify\n",
    "print(\"\\nüìã Project files:\")\n",
    "!ls -la *.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b671a",
   "metadata": {},
   "source": [
    "## üìä Step 2: Data Preparation and System Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11ce7b",
   "metadata": {},
   "source": [
    "## üîß New Features & Parameters\n",
    "\n",
    "### ‚ú® Enhanced BaseTrainer Features:\n",
    "\n",
    "1. **üìä Dataset Configuration**:\n",
    "   - `dataset_name`: Choose dataset version (e.g., \"phuhuy-se1/viGEC-v2\")\n",
    "   - `train_subset_ratio`: Use subset of training data (0.0-1.0) \n",
    "   - `validation_subset_ratio`: Use subset of validation data\n",
    "   - `test_subset_ratio`: Use subset of test data\n",
    "\n",
    "2. **üîç Customizable Search Space**:\n",
    "   - Define learning rate ranges\n",
    "   - Configure weight decay options\n",
    "   - Set batch size choices\n",
    "   - Customize warmup ratios\n",
    "\n",
    "3. **‚ö° Flexible Training Modes**:\n",
    "   - Hyperparameter optimization only\n",
    "   - Training with specific parameters\n",
    "   - Combined optimization + training\n",
    "\n",
    "### üí° Benefits:\n",
    "- **Faster experimentation** with data subsets\n",
    "- **Better hyperparameter control** \n",
    "- **Dataset version management**\n",
    "- **Memory-efficient training** for limited resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c36e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports and system readiness\n",
    "from data_utils import (\n",
    "    load_vigec_dataset, \n",
    "    get_model_and_tokenizer, \n",
    "    can_train_base_model,\n",
    "    check_dataset_format\n",
    ")\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Check system readiness\n",
    "console.print(\"[bold blue]üîç System Readiness Check[/bold blue]\")\n",
    "system_ready = can_train_base_model()\n",
    "\n",
    "# Check dataset format\n",
    "console.print(\"\\n[bold blue]üìã Dataset Format Check[/bold blue]\")\n",
    "dataset_ready = check_dataset_format()\n",
    "\n",
    "if system_ready and dataset_ready:\n",
    "    console.print(\"\\n[bold green]‚úÖ All checks passed! Ready to proceed.[/bold green]\")\n",
    "else:\n",
    "    console.print(\"\\n[bold red]‚ùå System not ready. Please check requirements.[/bold red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU setup check and optimization\n",
    "console.print(\"[bold blue]üöÄ Multi-GPU Setup Check[/bold blue]\")\n",
    "\n",
    "# Check multi-GPU availability\n",
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 1:\n",
    "    console.print(f\"[green]‚úÖ {device_count} GPUs detected![/green]\")\n",
    "    for i in range(device_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        memory_gb = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        console.print(f\"  GPU {i}: {gpu_name} ({memory_gb:.1f}GB)\")\n",
    "    \n",
    "    console.print(f\"[yellow]üí° Multi-GPU training will provide ~{device_count * 0.85:.1f}x speedup[/yellow]\")\n",
    "    console.print(f\"[blue]üìä Recommended batch size adjustment for {device_count} GPUs[/blue]\")\n",
    "    \n",
    "    # Test multi-GPU functionality\n",
    "    try:\n",
    "        from base_trainer import test_multi_gpu\n",
    "        console.print(\"[green]‚úÖ Multi-GPU support verified[/green]\")\n",
    "    except ImportError:\n",
    "        console.print(\"[yellow]‚ö†Ô∏è  Multi-GPU test not available, but should work fine[/yellow]\")\n",
    "        \n",
    "else:\n",
    "    console.print(f\"[blue]‚ÑπÔ∏è  Single GPU training: {torch.cuda.get_device_name()}[/blue]\")\n",
    "\n",
    "console.print(\"\\n[green]üîß System optimized for available hardware[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa031742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset with configurable parameters\n",
    "console.print(\"[bold blue]üìä Loading viGEC Dataset[/bold blue]\")\n",
    "\n",
    "# Dataset configuration - modify these as needed\n",
    "DATASET_CONFIG = {\n",
    "    \"dataset_name\": \"phuhuy-se1/viGEC-v2\",  # Change to \"phuhuy-se1/viGEC-v2\" for version 2\n",
    "    \"train_subset_ratio\": 1.0,  # Use 10% of training data for faster processing in Colab\n",
    "    \"validation_subset_ratio\": 1.0,  # Use 20% of validation data  \n",
    "    \"test_subset_ratio\": 0.5   # Use 5% of test data for faster evaluation\n",
    "}\n",
    "\n",
    "console.print(f\"[yellow]üìã Dataset Configuration:[/yellow]\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    console.print(f\"  {key}: {value}\")\n",
    "\n",
    "# Load dataset with configurable parameters\n",
    "data = load_vigec_dataset(\n",
    "    dataset_name=DATASET_CONFIG[\"dataset_name\"],\n",
    "    train_subset_ratio=DATASET_CONFIG[\"train_subset_ratio\"],\n",
    "    validation_subset_ratio=DATASET_CONFIG[\"validation_subset_ratio\"],\n",
    "    test_subset_ratio=DATASET_CONFIG[\"test_subset_ratio\"]\n",
    ")\n",
    "\n",
    "console.print(f\"\\n[green]Dataset loaded successfully![/green]\")\n",
    "for split, split_data in data.items():\n",
    "    console.print(f\"  {split}: {len(split_data)} samples\")\n",
    "    \n",
    "# Show subset ratios effect\n",
    "console.print(f\"\\n[blue]üìä Subset Effects:[/blue]\")\n",
    "console.print(f\"  Training samples: ~{len(data['train'])} (subset ratio: {DATASET_CONFIG['train_subset_ratio']})\")\n",
    "console.print(f\"  Validation samples: ~{len(data['validation'])} (subset ratio: {DATASET_CONFIG['validation_subset_ratio']})\")\n",
    "console.print(f\"  Test samples: ~{len(data['test'])} (subset ratio: {DATASET_CONFIG['test_subset_ratio']})\")\n",
    "\n",
    "# Save processed data\n",
    "from data_utils import save_processed_data\n",
    "save_processed_data(data, \"./data/processed\")\n",
    "console.print(\"\\n[blue]‚úÖ Data saved to ./data/processed/[/blue]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201dc15d",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: Model Selection and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cd00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your model - uncomment one of these:\n",
    "MODEL_NAME = \"vinai/bartpho-syllable\"  # Recommended for Vietnamese\n",
    "# MODEL_NAME = \"VietAI/vit5-base\"     # Alternative option\n",
    "# MODEL_NAME = \"VietAI/vit5-large\"    # Larger model (requires more GPU memory)\n",
    "\n",
    "console.print(f\"[bold blue]ü§ñ Loading Model: {MODEL_NAME}[/bold blue]\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = get_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "console.print(f\"[green]‚úÖ Model loaded successfully![/green]\")\n",
    "console.print(f\"  Model: {model.__class__.__name__}\")\n",
    "console.print(f\"  Tokenizer: {tokenizer.__class__.__name__}\")\n",
    "console.print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"T√¥i ƒëang h·ªçc ti·∫øng vi·ªát.\"\n",
    "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "console.print(f\"\\n[blue]üß™ Tokenization Test:[/blue]\")\n",
    "console.print(f\"  Input: {test_text}\")\n",
    "console.print(f\"  Tokens: {tokens['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39334726",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 4: Base Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bb88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "TRAINING_CONFIG = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"output_dir\": \"./models/base_model\",\n",
    "    \"max_epochs\": 3,  # Reduced for Colab\n",
    "    \"batch_size\": 8,  # Adjust based on GPU memory\n",
    "    \"use_wandb\": True,  # Set to False if you don't want to use Weights & Biases\n",
    "    \"run_optimization\": False,  # Set to True for hyperparameter optimization (takes longer)\n",
    "    \n",
    "    # New dataset parameters\n",
    "    \"dataset_name\": \"phuhuy-se1/viGEC\",  # Change to \"phuhuy-se1/viGEC-v2\" for version 2\n",
    "    \"train_subset_ratio\": 0.1,  # Use 10% of training data for faster training in Colab\n",
    "    \"validation_subset_ratio\": 0.2,  # Use 20% of validation data\n",
    "    \"test_subset_ratio\": 0.05,  # Use 5% of test data\n",
    "    \n",
    "    # Custom search space for hyperparameter optimization (if enabled)\n",
    "    \"search_space\": {\n",
    "        'learning_rate': {'low': 1e-5, 'high': 5e-4, 'log': True},\n",
    "        'weight_decay': {'low': 0.001, 'high': 0.05, 'log': True},\n",
    "        'label_smoothing': {'low': 0.0, 'high': 0.2},\n",
    "        'batch_size': [8, 16, 24],  # Smaller batch sizes for Colab\n",
    "        'warmup_ratio': {'low': 0.05, 'high': 0.15}\n",
    "    }\n",
    "}\n",
    "\n",
    "console.print(\"[bold blue]üèãÔ∏è Base Model Training Configuration:[/bold blue]\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    if key != \"search_space\":  # Don't print the search space dict for brevity\n",
    "        console.print(f\"  {key}: {value}\")\n",
    "\n",
    "if TRAINING_CONFIG[\"run_optimization\"]:\n",
    "    console.print(\"\\n[yellow]‚ö†Ô∏è Hyperparameter optimization enabled - this will take longer but may improve results[/yellow]\")\n",
    "    console.print(f\"[blue]Search space configured with {len(TRAINING_CONFIG['search_space'])} parameters[/blue]\")\n",
    "else:\n",
    "    console.print(\"\\n[blue]‚ÑπÔ∏è Using default parameters for faster training[/blue]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2820213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start base model training with improved error handling\n",
    "from base_trainer import BaseTrainer\n",
    "\n",
    "console.print(\"[bold green]üöÄ Starting Base Model Training...[/bold green]\")\n",
    "\n",
    "# Check and adjust batch size for multi-GPU\n",
    "original_batch_size = TRAINING_CONFIG[\"batch_size\"]\n",
    "device_count = torch.cuda.device_count()\n",
    "\n",
    "if device_count > 1:\n",
    "    # For multi-GPU, adjust batch size\n",
    "    adjusted_batch_size = max(1, original_batch_size // device_count)\n",
    "    console.print(f\"[yellow]üìä Multi-GPU batch size adjustment:[/yellow]\")\n",
    "    console.print(f\"  Original batch size: {original_batch_size}\")\n",
    "    console.print(f\"  Per-GPU batch size: {adjusted_batch_size}\")\n",
    "    console.print(f\"  Total effective batch size: {adjusted_batch_size * device_count}\")\n",
    "    final_batch_size = adjusted_batch_size\n",
    "else:\n",
    "    final_batch_size = original_batch_size\n",
    "    console.print(f\"[blue]üìä Single GPU batch size: {final_batch_size}[/blue]\")\n",
    "\n",
    "# Create base trainer with enhanced parameters\n",
    "try:\n",
    "    base_trainer = BaseTrainer(\n",
    "        model_name=TRAINING_CONFIG[\"model_name\"],\n",
    "        data_dir=\"./data/processed\",  # Use the processed data directory\n",
    "        output_dir=TRAINING_CONFIG[\"output_dir\"],\n",
    "        hyperopt=TRAINING_CONFIG[\"run_optimization\"],  # Enable/disable hyperopt\n",
    "        use_wandb=TRAINING_CONFIG[\"use_wandb\"],\n",
    "        \n",
    "        # New dataset parameters\n",
    "        dataset_name=TRAINING_CONFIG[\"dataset_name\"],\n",
    "        train_subset_ratio=TRAINING_CONFIG[\"train_subset_ratio\"],\n",
    "        validation_subset_ratio=TRAINING_CONFIG[\"validation_subset_ratio\"],\n",
    "        test_subset_ratio=TRAINING_CONFIG[\"test_subset_ratio\"]\n",
    "    )\n",
    "    \n",
    "    console.print(\"[green]‚úÖ BaseTrainer initialized successfully[/green]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    console.print(f\"[red]‚ùå Error initializing BaseTrainer: {e}[/red]\")\n",
    "    console.print(\"[yellow]üí° Trying with fallback configuration...[/yellow]\")\n",
    "    \n",
    "    # Fallback configuration\n",
    "    base_trainer = BaseTrainer(\n",
    "        model_name=TRAINING_CONFIG[\"model_name\"],\n",
    "        output_dir=TRAINING_CONFIG[\"output_dir\"],\n",
    "        hyperopt=False,  # Disable hyperopt for fallback\n",
    "        use_wandb=False  # Disable wandb for fallback\n",
    "    )\n",
    "\n",
    "# Train the model with error handling\n",
    "try:\n",
    "    # Check for multi-GPU setup and adjust batch size accordingly\n",
    "    device_count = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "    console.print(f\"[blue]üñ•Ô∏è  Available GPUs: {device_count}[/blue]\")\n",
    "    \n",
    "    # Adjust batch size for multi-GPU if needed\n",
    "    if device_count > 1:\n",
    "        # Each GPU gets a portion of the batch\n",
    "        per_gpu_batch_size = max(1, final_batch_size // device_count)\n",
    "        total_effective_batch_size = per_gpu_batch_size * device_count\n",
    "        console.print(f\"[yellow]üîÑ Multi-GPU detected: Using {per_gpu_batch_size} batch size per GPU[/yellow]\")\n",
    "        console.print(f\"[yellow]   Total effective batch size: {total_effective_batch_size}[/yellow]\")\n",
    "        final_batch_size = per_gpu_batch_size\n",
    "    \n",
    "    if ENABLE_HYPEROPT:\n",
    "        console.print(\"[yellow]üî¨ Running hyperparameter optimization (this will take longer)...[/yellow]\")\n",
    "        study = base_trainer.optimize_hyperparameters(\n",
    "            n_trials=N_TRIALS,\n",
    "            max_epochs=TRAINING_CONFIG[\"max_epochs\"],\n",
    "            base_batch_size=final_batch_size\n",
    "        )\n",
    "        \n",
    "        if study is not None:\n",
    "            console.print(f\"[green]‚úÖ Best parameters: {study.best_params}[/green]\")\n",
    "            console.print(f\"[green]‚úÖ Best F0.5 score: {study.best_value:.4f}[/green]\")\n",
    "            \n",
    "            # Train final model with best parameters  \n",
    "            console.print(\"[blue]üèÉ Training final model with best parameters...[/blue]\")\n",
    "            trained_model = base_trainer.train_with_params(\n",
    "                params=study.best_params,\n",
    "                max_epochs=TRAINING_CONFIG[\"max_epochs\"],\n",
    "                batch_size=study.best_params.get('batch_size', final_batch_size)\n",
    "            )\n",
    "        else:\n",
    "            console.print(\"[yellow]‚ö†Ô∏è  Hyperparameter optimization failed, using default training[/yellow]\")\n",
    "            trained_model = base_trainer.train(\n",
    "                max_epochs=TRAINING_CONFIG[\"max_epochs\"],\n",
    "                batch_size=final_batch_size\n",
    "            )\n",
    "    else:\n",
    "        console.print(\"[blue]üèÉ Training with default parameters...[/blue]\")\n",
    "        \n",
    "        # Train the model (hyperopt is controlled by the hyperopt parameter in constructor)\n",
    "        trained_model = base_trainer.train(\n",
    "            max_epochs=TRAINING_CONFIG[\"max_epochs\"],\n",
    "            batch_size=final_batch_size,\n",
    "            search_space=None  # No search space needed for default training\n",
    "        )\n",
    "\n",
    "    console.print(\"[bold green]‚úÖ Base model training completed![/bold green]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    console.print(f\"[red]‚ùå Training failed: {error_msg}[/red]\")\n",
    "    \n",
    "    # Specific error handling for common issues\n",
    "    if \"find_unused_parameters\" in error_msg:\n",
    "        console.print(\"[yellow]üîß Trainer argument error detected - this has been fixed in latest code[/yellow]\")\n",
    "        console.print(\"[blue]üí° The error is caused by DDP strategy arguments being passed to Trainer[/blue]\")\n",
    "        console.print(\"[green]‚úÖ Latest code filters these arguments automatically[/green]\")\n",
    "    elif \"CUDA out of memory\" in error_msg:\n",
    "        console.print(\"[yellow]üíæ GPU memory error - try reducing batch size[/yellow]\")\n",
    "        console.print(f\"[blue]Current batch size: {final_batch_size}[/blue]\")\n",
    "        console.print(\"[blue]üí° Try setting final_batch_size = 2 or 1[/blue]\")\n",
    "    elif \"No module named\" in error_msg:\n",
    "        console.print(\"[yellow]üì¶ Missing dependency - check package installation[/yellow]\")\n",
    "        console.print(\"[blue]üí° Try re-running the pip install cell above[/blue]\")\n",
    "    else:\n",
    "        console.print(\"[yellow]üí° This might be due to memory constraints or configuration issues[/yellow]\")\n",
    "        console.print(\"[blue]üîß Try reducing batch size or disabling hyperopt[/blue]\")\n",
    "    \n",
    "    # Show the error for debugging\n",
    "    import traceback\n",
    "    console.print(f\"[red]Error details: {traceback.format_exc()}[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d4aaa",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Negative Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70bb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative samples for contrastive learning\n",
    "from negative_sampler import NegativeSampler\n",
    "import os\n",
    "\n",
    "console.print(\"[bold blue]üéØ Generating Negative Samples...[/bold blue]\")\n",
    "\n",
    "# Create negative sampler (use the final model from training)\n",
    "base_model_path = os.path.join(TRAINING_CONFIG[\"output_dir\"], \"final_model\")\n",
    "\n",
    "# Check if trained model exists\n",
    "if os.path.exists(base_model_path):\n",
    "    console.print(f\"[green]‚úÖ Using trained model from {base_model_path}[/green]\")\n",
    "    model_path = base_model_path\n",
    "else:\n",
    "    console.print(f\"[yellow]‚ö†Ô∏è Trained model not found, using base model {MODEL_NAME}[/yellow]\")\n",
    "    model_path = MODEL_NAME\n",
    "\n",
    "negative_sampler = NegativeSampler(\n",
    "    model_path=model_path,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "# Generate negative samples for training data\n",
    "# Use smaller subset for Colab to avoid memory issues\n",
    "train_subset = data['train'][:1000] if len(data['train']) > 1000 else data['train']\n",
    "\n",
    "contrastive_data = negative_sampler.generate_contrastive_dataset(\n",
    "    data=train_subset,\n",
    "    num_negatives=3,  # Generate 3 negative samples per positive\n",
    "    output_file=\"./data/contrastive_train.json\"\n",
    ")\n",
    "\n",
    "console.print(f\"[green]‚úÖ Generated {len(contrastive_data)} contrastive samples![/green]\")\n",
    "console.print(\"[blue]üíæ Saved to ./data/contrastive_train.json[/blue]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622975a9",
   "metadata": {},
   "source": [
    "## üî• Step 6: Contrastive Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f588ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive learning training\n",
    "from contrastive_trainer import ContrastiveTrainer\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "console.print(\"[bold blue]üî• Starting Contrastive Learning Training...[/bold blue]\")\n",
    "\n",
    "# First, we need to prepare the contrastive data in the expected format\n",
    "contrastive_data_dir = \"./data/contrastive\"\n",
    "os.makedirs(contrastive_data_dir, exist_ok=True)\n",
    "\n",
    "# Convert the contrastive data to the expected format for validation\n",
    "validation_contrastive = []\n",
    "for item in data['validation'][:200]:  # Use subset for validation\n",
    "    validation_contrastive.append({\n",
    "        'source': item['source'],\n",
    "        'positive': item['target'],\n",
    "        'negatives': [item['source']]  # Simple negative sample\n",
    "    })\n",
    "\n",
    "# Save validation data\n",
    "with open(os.path.join(contrastive_data_dir, \"validation_contrastive.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(validation_contrastive, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Copy training contrastive data to the expected location\n",
    "if os.path.exists(\"./data/contrastive_train.json\"):\n",
    "    shutil.copy(\"./data/contrastive_train.json\", \n",
    "                os.path.join(contrastive_data_dir, \"train_contrastive.json\"))\n",
    "\n",
    "# Create contrastive trainer\n",
    "contrastive_trainer = ContrastiveTrainer(\n",
    "    base_model_path=os.path.join(TRAINING_CONFIG[\"output_dir\"], \"final_model\"),\n",
    "    contrastive_data_dir=contrastive_data_dir,\n",
    "    output_dir=\"./models/contrastive_model\",\n",
    "    hyperopt=False  # Disable hyperopt for faster training in Colab\n",
    ")\n",
    "\n",
    "# Train with contrastive learning\n",
    "contrastive_trainer.train()\n",
    "\n",
    "console.print(\"[bold green]‚úÖ Contrastive learning training completed![/bold green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f940b5f",
   "metadata": {},
   "source": [
    "## üß™ Step 7: Inference and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539896fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for inference with improved handling\n",
    "from inference import GECInference\n",
    "import os\n",
    "\n",
    "console.print(\"[bold blue]üß™ Setting up Inference...[/bold blue]\")\n",
    "\n",
    "# Determine which model to use for inference\n",
    "contrastive_model_path = \"./models/contrastive_model\"\n",
    "base_model_path = TRAINING_CONFIG[\"output_dir\"]\n",
    "\n",
    "# Check for final_model subdirectory\n",
    "final_model_path = os.path.join(base_model_path, \"final_model\")\n",
    "\n",
    "if os.path.exists(contrastive_model_path) and os.listdir(contrastive_model_path):\n",
    "    model_path = contrastive_model_path\n",
    "    console.print(f\"[green]‚úÖ Using contrastive model from {model_path}[/green]\")\n",
    "elif os.path.exists(final_model_path) and os.listdir(final_model_path):\n",
    "    model_path = final_model_path\n",
    "    console.print(f\"[yellow]‚ö†Ô∏è Using base model (final) from {model_path}[/yellow]\")\n",
    "elif os.path.exists(base_model_path) and os.listdir(base_model_path):\n",
    "    model_path = base_model_path\n",
    "    console.print(f\"[yellow]‚ö†Ô∏è Using base model from {model_path}[/yellow]\")\n",
    "else:\n",
    "    model_path = MODEL_NAME\n",
    "    console.print(f\"[blue]‚ÑπÔ∏è Using original model {model_path}[/blue]\")\n",
    "\n",
    "# Debug: Check what's in the model directory\n",
    "if model_path != MODEL_NAME:\n",
    "    console.print(f\"[dim]üìÅ Model directory contents:[/dim]\")\n",
    "    try:\n",
    "        for item in os.listdir(model_path):\n",
    "            console.print(f\"  - {item}\")\n",
    "    except:\n",
    "        console.print(\"  [red]Could not list directory[/red]\")\n",
    "\n",
    "# Create inference engine with proper error handling\n",
    "try:\n",
    "    console.print(f\"[yellow]üîÑ Loading inference engine from {model_path}...[/yellow]\")\n",
    "    \n",
    "    # Fixed: Remove model_name parameter which doesn't exist in GECInference\n",
    "    gec_inference = GECInference(\n",
    "        model_path=model_path,\n",
    "        use_contrastive_search=False,  # Disable for now to match training\n",
    "        device=\"auto\"\n",
    "    )\n",
    "    \n",
    "    console.print(\"[green]‚úÖ Inference engine ready![/green]\")\n",
    "    \n",
    "    # Test the inference with a simple example\n",
    "    test_input = \"T√¥i ƒëang h·ªçc ti·∫øng vi·ªát.\"\n",
    "    console.print(f\"[blue]üß™ Testing inference:[/blue]\")\n",
    "    console.print(f\"  Input: {test_input}\")\n",
    "    \n",
    "    try:\n",
    "        test_output = gec_inference.correct_text(test_input)\n",
    "        console.print(f\"  Output: {test_output}\")\n",
    "        console.print(\"[green]‚úÖ Inference test successful![/green]\")\n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå Inference test failed: {e}[/red]\")\n",
    "        console.print(\"[yellow]üí° This might indicate a model loading issue[/yellow]\")\n",
    "    \n",
    "    # Check if model has task prefix (for ViT5 models)\n",
    "    if hasattr(gec_inference.tokenizer, 'task_prefix'):\n",
    "        console.print(f\"[blue]üè∑Ô∏è  Task prefix detected: '{gec_inference.tokenizer.task_prefix}'[/blue]\")\n",
    "    elif 'vit5' in MODEL_NAME.lower() or 'mt5' in MODEL_NAME.lower():\n",
    "        # Manually set task prefix for ViT5/mT5 models\n",
    "        gec_inference.tokenizer.task_prefix = \"grammar: \"\n",
    "        console.print(f\"[yellow]üè∑Ô∏è  Manually set task prefix: '{gec_inference.tokenizer.task_prefix}'[/yellow]\")\n",
    "    else:\n",
    "        console.print(\"[blue]‚ÑπÔ∏è  No task prefix needed for this model[/blue]\")\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[red]‚ùå Failed to load inference engine: {e}[/red]\")\n",
    "    console.print(\"[yellow]üí° Falling back to original model...[/yellow]\")\n",
    "    \n",
    "    # Fixed: Remove model_name parameter in fallback too\n",
    "    try:\n",
    "        gec_inference = GECInference(\n",
    "            model_path=MODEL_NAME,\n",
    "            use_contrastive_search=False,\n",
    "            device=\"auto\"\n",
    "        )\n",
    "        console.print(\"[yellow]‚ö†Ô∏è Using fallback original model[/yellow]\")\n",
    "    except Exception as fallback_error:\n",
    "        console.print(f\"[red]‚ùå Fallback also failed: {fallback_error}[/red]\")\n",
    "        console.print(\"[red]üí• Cannot proceed with inference. Check model and dependencies.[/red]\")\n",
    "        \n",
    "        # Show detailed error information\n",
    "        import traceback\n",
    "        console.print(f\"[red]Detailed error: {traceback.format_exc()}[/red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba974962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Investigate training vs evaluation discrepancy\n",
    "console.print(\"[bold red]üîç DEBUGGING: Training vs Evaluation Discrepancy[/bold red]\")\n",
    "\n",
    "console.print(\"[yellow]üìä Analysis of the 91% vs 34% discrepancy:[/yellow]\")\n",
    "console.print(\"1. Training loss ‚â† F0.5 evaluation metric\")\n",
    "console.print(\"2. Different preprocessing during training vs inference\")\n",
    "console.print(\"3. Task prefix issues with ViT5/mT5 models\")\n",
    "console.print(\"4. Tokenization differences\")\n",
    "\n",
    "# Test 1: Check training data format vs inference format\n",
    "console.print(\"\\n[blue]üß™ Test 1: Data Format Comparison[/blue]\")\n",
    "if 'data' in locals():\n",
    "    sample_source = data['validation'][0]['source']\n",
    "    sample_target = data['validation'][0]['target']\n",
    "    \n",
    "    console.print(f\"Training source: '{sample_source}'\")\n",
    "    console.print(f\"Training target: '{sample_target}'\")\n",
    "    \n",
    "    # Test how inference processes this\n",
    "    inference_result = gec_inference.correct_text(sample_source)\n",
    "    console.print(f\"Inference result: '{inference_result}'\")\n",
    "    \n",
    "    # Check if task prefix was used during training\n",
    "    if hasattr(gec_inference.tokenizer, 'task_prefix'):\n",
    "        prefix = gec_inference.tokenizer.task_prefix\n",
    "        console.print(f\"Task prefix: '{prefix}'\")\n",
    "        \n",
    "        # Test with manual prefix\n",
    "        manual_prefix_input = prefix + sample_source\n",
    "        console.print(f\"Manual prefix input: '{manual_prefix_input}'\")\n",
    "        \n",
    "        # Check tokenization\n",
    "        tokens_without_prefix = gec_inference.tokenizer(sample_source, return_tensors=\"pt\")\n",
    "        tokens_with_prefix = gec_inference.tokenizer(manual_prefix_input, return_tensors=\"pt\")\n",
    "        \n",
    "        console.print(f\"Tokens without prefix length: {tokens_without_prefix['input_ids'].shape}\")\n",
    "        console.print(f\"Tokens with prefix length: {tokens_with_prefix['input_ids'].shape}\")\n",
    "\n",
    "# Test 2: Check evaluation metric calculation\n",
    "console.print(\"\\n[blue]üß™ Test 2: Evaluation Metric Check[/blue]\")\n",
    "from evaluator import F05Evaluator\n",
    "\n",
    "# Create evaluator\n",
    "eval_test = F05Evaluator()\n",
    "\n",
    "# Test on a simple example where we know the answer\n",
    "test_source = \"T√¥i ƒëang h·ªçc ti·∫øng vi·ªát\"\n",
    "test_target = \"T√¥i ƒëang h·ªçc ti·∫øng Vi·ªát\"  # Simple capitalization fix\n",
    "test_prediction = \"T√¥i ƒëang h·ªçc ti·∫øng Vi·ªát\"  # Perfect prediction\n",
    "\n",
    "f05_perfect = eval_test.calculate_f05(test_source, test_prediction, test_target)\n",
    "console.print(f\"Perfect prediction F0.5: {f05_perfect:.4f}\")\n",
    "\n",
    "# Test with a wrong prediction\n",
    "test_prediction_wrong = \"T√¥i ƒëang h·ªçc ti·∫øng anh\"  # Wrong prediction\n",
    "f05_wrong = eval_test.calculate_f05(test_source, test_prediction_wrong, test_target)\n",
    "console.print(f\"Wrong prediction F0.5: {f05_wrong:.4f}\")\n",
    "\n",
    "# Test with no change (common issue)\n",
    "test_prediction_no_change = test_source  # No change\n",
    "f05_no_change = eval_test.calculate_f05(test_source, test_prediction_no_change, test_target)\n",
    "console.print(f\"No change prediction F0.5: {f05_no_change:.4f}\")\n",
    "\n",
    "console.print(\"\\n[yellow]üí° Common issues that cause this discrepancy:[/yellow]\")\n",
    "console.print(\"1. Model not making enough changes (conservative predictions)\")\n",
    "console.print(\"2. Task prefix missing during inference\")\n",
    "console.print(\"3. Different loss function vs evaluation metric\")\n",
    "console.print(\"4. Model trained on different data format than inference expects\")\n",
    "\n",
    "# Test 3: Loss vs F0.5 relationship\n",
    "console.print(\"\\n[blue]üß™ Test 3: Understanding the Loss-F0.5 Gap[/blue]\")\n",
    "console.print(\"Training loss is cross-entropy loss on token predictions\")\n",
    "console.print(\"F0.5 measures edit-level precision/recall at word level\")\n",
    "console.print(\"A model can have low token-level loss but poor edit-level performance\")\n",
    "\n",
    "console.print(\"\\n[red]üéØ RECOMMENDATIONS TO FIX:[/red]\")\n",
    "console.print(\"1. Ensure task prefix is used consistently\")\n",
    "console.print(\"2. Check model is actually making corrections (not just copying)\")\n",
    "console.print(\"3. Verify training data preprocessing matches inference\")\n",
    "console.print(\"4. Consider using F0.5 as training metric instead of cross-entropy\")\n",
    "\n",
    "# Evaluate on test set with consistency checking\n",
    "from evaluator import F05Evaluator\n",
    "import numpy as np\n",
    "\n",
    "console.print(\"[bold blue]üìä Evaluating on Test Set...[/bold blue]\")\n",
    "\n",
    "# Important note about validation vs evaluation consistency\n",
    "console.print(\"[bold yellow]‚ö†Ô∏è  Important Note: Validation Consistency[/bold yellow]\")\n",
    "console.print(\"[blue]If training validation shows high metrics (e.g., 91%) but evaluation shows low metrics (e.g., 34%),[/blue]\")\n",
    "console.print(\"[blue]this is usually due to differences in:[/blue]\")\n",
    "console.print(\"[blue]  1. Task prefix handling (ViT5/mT5 models need 'grammar: ' prefix)[/blue]\")\n",
    "console.print(\"[blue]  2. Generation parameters (num_beams, etc.)[/blue]\")\n",
    "console.print(\"[blue]  3. Validation subset vs full evaluation[/blue]\")\n",
    "console.print(\"[green]‚úÖ Latest code fixes these inconsistencies in base_trainer.py[/green]\")\n",
    "\n",
    "# Create evaluator - check if we need to pass tokenizer\n",
    "try:\n",
    "    # Try with tokenizer first\n",
    "    evaluator = F05Evaluator(tokenizer=gec_inference.tokenizer)\n",
    "except:\n",
    "    # Fallback to no tokenizer\n",
    "    evaluator = F05Evaluator()\n",
    "\n",
    "# Evaluate on test set (using subset for faster evaluation)\n",
    "test_data_subset = data['test'][:100]  # Use 100 samples for evaluation\n",
    "sources = [item['source'] for item in test_data_subset]\n",
    "references = [item['target'] for item in test_data_subset]\n",
    "\n",
    "# Generate predictions\n",
    "console.print(\"[yellow]üîÆ Generating predictions...[/yellow]\")\n",
    "predictions = []\n",
    "for i, source in enumerate(sources):\n",
    "    if i % 20 == 0:  # Progress indicator\n",
    "        console.print(f\"[blue]Processing {i+1}/{len(sources)}...[/blue]\")\n",
    "    pred = gec_inference.correct_text(source)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Calculate metrics\n",
    "console.print(\"[yellow]üìà Calculating metrics...[/yellow]\")\n",
    "try:\n",
    "    # Try batch evaluation first\n",
    "    results = evaluator.evaluate_batch(predictions, references, sources)\n",
    "except AttributeError:\n",
    "    # Fallback to individual evaluation\n",
    "    f05_scores = []\n",
    "    for pred, ref, src in zip(predictions, references, sources):\n",
    "        f05 = evaluator.calculate_f05(src, pred, ref)\n",
    "        f05_scores.append(f05)\n",
    "    \n",
    "    results = {\n",
    "        \"f05_score\": np.mean(f05_scores),\n",
    "        \"num_samples\": len(f05_scores)\n",
    "    }\n",
    "\n",
    "console.print(\"\\n[bold green]üìà Evaluation Results:[/bold green]\")\n",
    "for metric, value in results.items():\n",
    "    if isinstance(value, float):\n",
    "        console.print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        console.print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Validation consistency check\n",
    "avg_f05 = results.get(\"f05_score\", 0.0)\n",
    "console.print(f\"\\n[bold cyan]üîç Consistency Analysis:[/bold cyan]\")\n",
    "console.print(f\"[blue]Evaluation F0.5 Score: {avg_f05:.4f} ({avg_f05*100:.1f}%)[/blue]\")\n",
    "\n",
    "if avg_f05 < 0.5:  # Less than 50%\n",
    "    console.print(\"[yellow]‚ö†Ô∏è  Low evaluation score detected![/yellow]\")\n",
    "    console.print(\"[blue]üí° If training validation showed much higher scores, this suggests:[/blue]\")\n",
    "    console.print(\"   1. Task prefix was missing during training validation\")\n",
    "    console.print(\"   2. Generation parameters differed between training and inference\") \n",
    "    console.print(\"   3. Training validation used subset, evaluation uses different data\")\n",
    "    console.print(\"[green]‚úÖ Use the updated base_trainer.py for consistent metrics[/green]\")\n",
    "elif avg_f05 > 0.8:  # Greater than 80%\n",
    "    console.print(\"[green]‚úÖ Good evaluation score! Training validation was likely accurate.[/green]\")\n",
    "else:\n",
    "    console.print(\"[yellow]üìä Moderate evaluation score. Check if this matches training validation.[/yellow]\")\n",
    "\n",
    "# Show some examples\n",
    "console.print(\"\\n[bold cyan]üîç Sample Results:[/bold cyan]\")\n",
    "for i in range(min(5, len(sources))):\n",
    "    f05_single = evaluator.calculate_f05(sources[i], predictions[i], references[i])\n",
    "    console.print(f\"\\n{i+1}. Source: {sources[i]}\")\n",
    "    console.print(f\"   Target: {references[i]}\")\n",
    "    console.print(f\"   Prediction: {predictions[i]}\")\n",
    "    console.print(f\"   F0.5 Score: {f05_single:.4f}\")\n",
    "\n",
    "# Additional debugging option\n",
    "console.print(\"\\n[bold blue]üîß Debug Option:[/bold blue]\")\n",
    "console.print(\"[yellow]To check validation consistency, run:[/yellow]\")\n",
    "console.print(\"[dim]python validation_consistency_checker.py[/dim]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing\n",
    "console.print(\"[bold blue]üéÆ Interactive Testing[/bold blue]\")\n",
    "\n",
    "# Test samples\n",
    "test_sentences = [\n",
    "    \"T√¥i ƒëang h·ªçc ti·∫øng vi·ªát ·ªü tr∆∞·ªùng ƒë·∫°i h·ªçc.\",\n",
    "    \"H√¥m nay tr·ªùi r·∫•t ƒë·∫πp v√† t√¥i mu·ªën ƒëi ch∆°i.\",\n",
    "    \"C√¥ ·∫•y l√†m vi·ªác t·∫°i m·ªôt c√¥ng ty l·ªõn ·ªü H√† N·ªôi.\",\n",
    "    \"Ch√∫ng t√¥i s·∫Ω ƒëi du l·ªãch v√†o cu·ªëi tu·∫ßn n√†y.\"\n",
    "]\n",
    "\n",
    "console.print(\"\\n[yellow]üìù Test Results:[/yellow]\")\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    corrected = gec_inference.correct_text(sentence)\n",
    "    console.print(f\"\\n{i}. Original: {sentence}\")\n",
    "    console.print(f\"   Corrected: {corrected}\")\n",
    "\n",
    "# Custom input\n",
    "console.print(\"\\n[bold cyan]‚úèÔ∏è Try your own text:[/bold cyan]\")\n",
    "print(\"Enter Vietnamese text to correct (or 'quit' to exit):\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"> \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    corrected = gec_inference.correct_text(user_input)\n",
    "    print(f\"Corrected: {corrected}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation with all metrics\n",
    "from evaluator import GECEvaluator\n",
    "import numpy as np\n",
    "\n",
    "console.print(\"[bold blue]üìä Comprehensive Evaluation with All Metrics[/bold blue]\")\n",
    "\n",
    "console.print(\"[yellow]üîç Metrics to be calculated:[/yellow]\")\n",
    "console.print(\"  ‚Ä¢ F0.5 (Edit-level, precision-weighted)\")\n",
    "console.print(\"  ‚Ä¢ Precision & Recall (Edit-level)\")  \n",
    "console.print(\"  ‚Ä¢ BLEU (Traditional n-gram overlap)\")\n",
    "console.print(\"  ‚Ä¢ GLEU (Better for GEC tasks)\")\n",
    "console.print(\"  ‚Ä¢ ROUGE-1, ROUGE-2, ROUGE-L (Token overlap)\")\n",
    "console.print(\"  ‚Ä¢ Input-preserving Edit Ratio\")\n",
    "\n",
    "# Create comprehensive evaluator\n",
    "evaluator = GECEvaluator(tokenizer=gec_inference.tokenizer)\n",
    "\n",
    "# Evaluate on test set (using subset for faster evaluation)\n",
    "test_data_subset = data['test'][:100]  # Use 100 samples for evaluation\n",
    "sources = [item['source'] for item in test_data_subset]\n",
    "references = [item['target'] for item in test_data_subset]\n",
    "\n",
    "console.print(f\"[blue]üìã Evaluating on {len(sources)} samples...[/blue]\")\n",
    "\n",
    "# Generate predictions\n",
    "console.print(\"[yellow]üîÆ Generating predictions...[/yellow]\")\n",
    "predictions = []\n",
    "for i, source in enumerate(sources):\n",
    "    if i % 20 == 0:  # Progress indicator\n",
    "        console.print(f\"[blue]Processing {i+1}/{len(sources)}...[/blue]\")\n",
    "    pred = gec_inference.correct_text(source)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Calculate all metrics\n",
    "console.print(\"[yellow]üìà Calculating comprehensive metrics...[/yellow]\")\n",
    "comprehensive_results = evaluator.calculate_all_metrics(\n",
    "    sources=sources,\n",
    "    predictions=predictions,\n",
    "    targets=references,\n",
    "    print_results=True  # This will print a nice table\n",
    ")\n",
    "\n",
    "# Additional analysis\n",
    "console.print(\"\\n[bold cyan]üîç Detailed Analysis:[/bold cyan]\")\n",
    "\n",
    "# Show distribution of F0.5 scores\n",
    "from evaluator import F05Evaluator\n",
    "f05_evaluator = F05Evaluator()\n",
    "individual_f05_scores = []\n",
    "for src, pred, ref in zip(sources, predictions, references):\n",
    "    f05 = f05_evaluator.calculate_f05(src, pred, ref)\n",
    "    individual_f05_scores.append(f05)\n",
    "\n",
    "f05_array = np.array(individual_f05_scores)\n",
    "console.print(f\"[blue]üìä F0.5 Score Distribution:[/blue]\")\n",
    "console.print(f\"  Mean: {f05_array.mean():.4f}\")\n",
    "console.print(f\"  Std:  {f05_array.std():.4f}\")\n",
    "console.print(f\"  Min:  {f05_array.min():.4f}\")\n",
    "console.print(f\"  Max:  {f05_array.max():.4f}\")\n",
    "\n",
    "# Count perfect predictions\n",
    "perfect_predictions = sum(1 for f05 in individual_f05_scores if f05 == 1.0)\n",
    "console.print(f\"  Perfect predictions: {perfect_predictions}/{len(individual_f05_scores)} ({perfect_predictions/len(individual_f05_scores)*100:.1f}%)\")\n",
    "\n",
    "# Count no-change predictions\n",
    "no_change_predictions = sum(1 for src, pred in zip(sources, predictions) if src.strip() == pred.strip())\n",
    "console.print(f\"  No-change predictions: {no_change_predictions}/{len(sources)} ({no_change_predictions/len(sources)*100:.1f}%)\")\n",
    "\n",
    "# Show some examples categorized by performance\n",
    "console.print(\"\\n[bold cyan]üîç Sample Results by Performance:[/bold cyan]\")\n",
    "\n",
    "# Sort examples by F0.5 score\n",
    "examples_with_scores = list(zip(sources, predictions, references, individual_f05_scores))\n",
    "examples_with_scores.sort(key=lambda x: x[3], reverse=True)\n",
    "\n",
    "# Best examples\n",
    "console.print(\"\\n[green]üèÜ Best Examples (F0.5 ‚â• 0.8):[/green]\")\n",
    "best_examples = [ex for ex in examples_with_scores if ex[3] >= 0.8]\n",
    "for i, (src, pred, ref, score) in enumerate(best_examples[:3]):\n",
    "    console.print(f\"{i+1}. Source:     {src}\")\n",
    "    console.print(f\"   Target:     {ref}\")\n",
    "    console.print(f\"   Prediction: {pred}\")\n",
    "    console.print(f\"   F0.5:       {score:.4f}\")\n",
    "    console.print()\n",
    "\n",
    "# Worst examples\n",
    "console.print(\"[red]üîç Challenging Examples (F0.5 ‚â§ 0.2):[/red]\")\n",
    "worst_examples = [ex for ex in examples_with_scores if ex[3] <= 0.2]\n",
    "for i, (src, pred, ref, score) in enumerate(worst_examples[:3]):\n",
    "    console.print(f\"{i+1}. Source:     {src}\")\n",
    "    console.print(f\"   Target:     {ref}\")\n",
    "    console.print(f\"   Prediction: {pred}\")\n",
    "    console.print(f\"   F0.5:       {score:.4f}\")\n",
    "    console.print()\n",
    "\n",
    "# Save comprehensive results\n",
    "import json\n",
    "comprehensive_results['individual_f05_scores'] = individual_f05_scores\n",
    "comprehensive_results['examples'] = {\n",
    "    'best': [{'source': ex[0], 'prediction': ex[1], 'target': ex[2], 'f05': ex[3]} \n",
    "             for ex in best_examples[:5]],\n",
    "    'worst': [{'source': ex[0], 'prediction': ex[1], 'target': ex[2], 'f05': ex[3]} \n",
    "              for ex in worst_examples[:5]]\n",
    "}\n",
    "\n",
    "console.print(f\"\\n[blue]üíæ Comprehensive results saved to variable 'comprehensive_results'[/blue]\")\n",
    "console.print(\"[green]‚úÖ Evaluation completed with all metrics![/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b2389",
   "metadata": {},
   "source": [
    "## üíæ Step 8: Save and Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db60851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and create export package\n",
    "import json\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "console.print(\"[bold blue]üíæ Saving Results and Creating Export Package...[/bold blue]\")\n",
    "\n",
    "# Create results summary\n",
    "results_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"training_config\": TRAINING_CONFIG,\n",
    "    \"evaluation_results\": results,\n",
    "    \"test_samples\": len(test_data),\n",
    "    \"model_paths\": {\n",
    "        \"base_model\": TRAINING_CONFIG[\"output_dir\"],\n",
    "        \"contrastive_model\": \"./models/contrastive_model\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(\"./results_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "console.print(\"[green]‚úÖ Results saved to ./results_summary.json[/green]\")\n",
    "\n",
    "# Create downloadable package\n",
    "console.print(\"[yellow]üì¶ Creating export package...[/yellow]\")\n",
    "\n",
    "with zipfile.ZipFile(\"vietnamese_gec_models.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add results\n",
    "    zipf.write(\"results_summary.json\")\n",
    "    \n",
    "    # Add model files (if they exist)\n",
    "    import glob\n",
    "    for model_file in glob.glob(\"./models/**/*.bin\", recursive=True):\n",
    "        zipf.write(model_file)\n",
    "    for config_file in glob.glob(\"./models/**/config.json\", recursive=True):\n",
    "        zipf.write(config_file)\n",
    "    \n",
    "    # Add data samples\n",
    "    if os.path.exists(\"./data/contrastive_train.json\"):\n",
    "        zipf.write(\"./data/contrastive_train.json\")\n",
    "\n",
    "console.print(\"[bold green]üéâ Export package created: vietnamese_gec_models.zip[/bold green]\")\n",
    "console.print(\"[blue]üìÅ You can download this file from the Colab file browser[/blue]\")\n",
    "\n",
    "# Display final summary\n",
    "console.print(\"\\n[bold cyan]üèÜ Training Pipeline Completed Successfully![/bold cyan]\")\n",
    "console.print(f\"[green]‚úÖ Base model trained and saved[/green]\")\n",
    "console.print(f\"[green]‚úÖ Contrastive learning applied[/green]\")\n",
    "console.print(f\"[green]‚úÖ Model evaluated on test set[/green]\")\n",
    "console.print(f\"[green]‚úÖ Results exported for download[/green]\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
