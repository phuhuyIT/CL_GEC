{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac4ccfc",
   "metadata": {},
   "source": [
    "# üáªüá≥ Vietnamese GEC with Contrastive Learning - Google Colab (BARTpho Fixed)\n",
    "\n",
    "**‚úÖ Bug Fix Applied:** Fixed `'BartphoTokenizer' object has no attribute 'vocab'` error\n",
    "\n",
    "Complete pipeline for training Vietnamese Grammatical Error Correction models with Contrastive Learning.\n",
    "\n",
    "## üêõ Recent Fixes:\n",
    "- **BARTpho Tokenizer Fix**: Resolved vocabulary access issue for SentencePiece tokenizers\n",
    "- **AdamW Import Fix**: Fixed `ImportError: cannot import name 'AdamW' from 'transformers'` \n",
    "- **PyTorch Lightning**: Added missing Lightning installation to dependencies\n",
    "- **Improved Compatibility**: Better support for both BARTpho and ViT5 models\n",
    "- **Error Handling**: Added safe vocabulary checking methods\n",
    "\n",
    "## üìã Pipeline Overview:\n",
    "1. **Setup & Installation** - Install dependencies and create project structure\n",
    "2. **Data Preparation** - Load and preprocess viGEC dataset  \n",
    "3. **Base Model Training** - Fine-tune BARTpho/ViT5 with hyperparameter optimization\n",
    "4. **Negative Sample Generation** - Generate negative samples for contrastive learning\n",
    "5. **Contrastive Learning Training** - Train with contrastive loss + R-Drop\n",
    "6. **Inference & Evaluation** - Test and evaluate the model\n",
    "\n",
    "‚è∞ **Estimated Total Time**: 4-9 hours (depending on GPU)\n",
    "üîß **BARTpho Issue**: RESOLVED ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e46c94",
   "metadata": {},
   "source": [
    "## üöÄ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c159bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages - FIXED VERSIONS FOR COMPATIBILITY\n",
    "print(\"üì¶ Installing compatible package versions...\")\n",
    "\n",
    "# Install packages in correct order to avoid conflicts\n",
    "!pip install \"numpy>=1.21.0,<2.0.0\"  # Install numpy < 2.0 to avoid wandb conflicts\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers==4.36.0 datasets==2.15.0 accelerate==0.25.0\n",
    "!pip install sentencepiece tokenizers nltk sacrebleu evaluate rouge-score\n",
    "!pip install pandas scikit-learn tqdm rich omegaconf\n",
    "!pip install underthesea pyvi ipywidgets matplotlib seaborn\n",
    "!pip install \"optuna>=3.4.0,<4.0.0\" \"wandb>=0.16.0,<0.17.0\"\n",
    "!pip install \"lightning>=2.0.0\" \"pytorch-lightning>=2.0.0\"  # Fixed: Added PyTorch Lightning\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "\n",
    "# Create the COMPLETE fixed data_utils.py file\n",
    "data_utils_complete = '''\"\"\"\n",
    "Data utilities for Vietnamese GEC with viGEC dataset - COMPLETE FIXED VERSION\n",
    "Resolves: 'BartphoTokenizer' object has no attribute 'vocab' error\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration, T5Tokenizer\n",
    "import logging\n",
    "import json\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "import numpy as np\n",
    "\n",
    "console = Console()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ViGECDataset(Dataset):\n",
    "    \"\"\"Vietnamese GEC Dataset for training\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[Dict],\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 384,\n",
    "        is_train: bool = True\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_train = is_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        source = item['source']\n",
    "        target = item['target']\n",
    "        \n",
    "        # Add task prefix for ViT5\n",
    "        if hasattr(self.tokenizer, 'task_prefix'):\n",
    "            source = self.tokenizer.task_prefix + source\n",
    "        \n",
    "        # Tokenize source\n",
    "        source_encoding = self.tokenizer(\n",
    "            source,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize target\n",
    "        target_encoding = self.tokenizer(\n",
    "            target,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': source_encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': source_encoding['attention_mask'].squeeze(),\n",
    "            'labels': target_encoding['input_ids'].squeeze(),\n",
    "            'decoder_attention_mask': target_encoding['attention_mask'].squeeze(),\n",
    "            'source_text': source,\n",
    "            'target_text': target\n",
    "        }\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    \"\"\"Dataset for contrastive learning with positive/negative pairs\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[Dict],\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 384\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        source = item['source']\n",
    "        positive = item.get('positive', item.get('target'))  # gold target\n",
    "        negatives = item.get('negatives', [])  # list of negative samples\n",
    "        \n",
    "        # Add task prefix for ViT5\n",
    "        if hasattr(self.tokenizer, 'task_prefix'):\n",
    "            source = self.tokenizer.task_prefix + source\n",
    "        \n",
    "        # Tokenize source\n",
    "        source_encoding = self.tokenizer(\n",
    "            source,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize positive\n",
    "        positive_encoding = self.tokenizer(\n",
    "            positive,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize negatives (ensure we have at least 3)\n",
    "        negative_encodings = []\n",
    "        for neg in negatives[:3]:  # Use up to 3 negatives\n",
    "            neg_encoding = self.tokenizer(\n",
    "                neg,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            negative_encodings.append(neg_encoding)\n",
    "        \n",
    "        # Pad with duplicates if we don't have enough negatives\n",
    "        while len(negative_encodings) < 3:\n",
    "            negative_encodings.append(positive_encoding)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': source_encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': source_encoding['attention_mask'].squeeze(),\n",
    "            'positive_ids': positive_encoding['input_ids'].squeeze(),\n",
    "            'positive_attention_mask': positive_encoding['attention_mask'].squeeze(),\n",
    "            'negative_ids': torch.stack([neg['input_ids'].squeeze() for neg in negative_encodings]),\n",
    "            'negative_attention_mask': torch.stack([neg['attention_mask'].squeeze() for neg in negative_encodings]),\n",
    "            'source_text': source,\n",
    "            'positive_text': positive,\n",
    "            'negative_texts': negatives\n",
    "        }\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize Vietnamese text to UTF-8 NFC\"\"\"\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and preprocess Vietnamese text\"\"\"\n",
    "    text = normalize_text(text)\n",
    "    # Remove special characters but keep Vietnamese diacritics\n",
    "    text = re.sub(r'[^\\\\w\\\\s\\\\u00C0-\\\\u1EF9\\\\u0300-\\\\u036F.,!?;:()\"\\\\'\\\\'-]', '', text)\n",
    "    # Fix spacing around punctuation\n",
    "    text = re.sub(r'\\\\s*([.,!?;:])\\\\s*', r'\\\\1 ', text)\n",
    "    text = re.sub(r'\\\\s*([()\"\\\\'\\\\'])\\\\s*', r' \\\\1 ', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_vigec_dataset(\n",
    "    dataset_name: str = \"phuhuy-se1/viGEC\",\n",
    "    cache_dir: Optional[str] = None,\n",
    "    test_subset_ratio: float = 0.05\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Load and preprocess viGEC dataset\"\"\"\n",
    "    \n",
    "    console.print(f\"[bold blue]Loading dataset: {dataset_name}[/bold blue]\")\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå Error loading dataset: {e}[/red]\")\n",
    "        raise\n",
    "    \n",
    "    processed_data = {}\n",
    "    \n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        if split in dataset:\n",
    "            console.print(f\"[yellow]Processing {split} split...[/yellow]\")\n",
    "            \n",
    "            split_data = []\n",
    "            for item in track(dataset[split], description=f\"Processing {split}\"):\n",
    "                # Handle different column names\n",
    "                source = item.get('incorrect_text', item.get('source', ''))\n",
    "                target = item.get('correct_text', item.get('target', ''))\n",
    "                \n",
    "                if isinstance(source, str) and isinstance(target, str):\n",
    "                    source = clean_text(source)\n",
    "                    target = clean_text(target)\n",
    "                    \n",
    "                    # Skip empty or very short texts\n",
    "                    if len(source.split()) < 2 or len(target.split()) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    split_data.append({\n",
    "                        'source': source,\n",
    "                        'target': target,\n",
    "                        'id': item.get('id', len(split_data))\n",
    "                    })\n",
    "            \n",
    "            # For test split, use only a subset for faster evaluation\n",
    "            if split == 'test' and test_subset_ratio < 1.0:\n",
    "                import random\n",
    "                random.seed(42)  # For reproducibility\n",
    "                subset_size = int(len(split_data) * test_subset_ratio)\n",
    "                split_data = random.sample(split_data, subset_size)\n",
    "                console.print(f\"[blue]Using {subset_size} samples ({test_subset_ratio*100:.1f}%) from test set[/blue]\")\n",
    "            \n",
    "            processed_data[split] = split_data\n",
    "            console.print(f\"[green]{split}: {len(split_data)} samples[/green]\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def get_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"Get model and tokenizer for Vietnamese GEC - COMPLETE FIXED VERSION\"\"\"\n",
    "    \n",
    "    console.print(f\"[bold blue]Loading model: {model_name}[/bold blue]\")\n",
    "    \n",
    "    try:\n",
    "        if 'bartpho' in model_name.lower():\n",
    "            # BARTpho models\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "            console.print(\"[green]‚úÖ BARTpho model loaded[/green]\")\n",
    "            \n",
    "        elif 'vit5' in model_name.lower():\n",
    "            # ViT5 models\n",
    "            tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "            model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "            \n",
    "            # Add task prefix for ViT5\n",
    "            if not hasattr(tokenizer, 'task_prefix'):\n",
    "                tokenizer.task_prefix = \"grammatical error correction: \"\n",
    "                console.print(f\"[yellow]Added ViT5 task prefix: {tokenizer.task_prefix}[/yellow]\")\n",
    "            \n",
    "            console.print(\"[green]‚úÖ ViT5 model loaded[/green]\")\n",
    "            \n",
    "        else:\n",
    "            # Generic seq2seq models\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "            console.print(\"[green]‚úÖ Generic seq2seq model loaded[/green]\")\n",
    "        \n",
    "        # FIXED: Safe vocabulary checking for different tokenizer types\n",
    "        special_tokens = ['<gec>', '</gec>']\n",
    "        \n",
    "        try:\n",
    "            # Method 1: Standard vocab attribute (BERT, GPT, etc.)\n",
    "            if hasattr(tokenizer, 'vocab'):\n",
    "                vocab = tokenizer.vocab\n",
    "                console.print(\"[blue]Using .vocab attribute[/blue]\")\n",
    "                \n",
    "            # Method 2: get_vocab() method (SentencePiece, BARTpho, etc.)\n",
    "            elif hasattr(tokenizer, 'get_vocab'):\n",
    "                vocab = tokenizer.get_vocab()\n",
    "                console.print(f\"[blue]Using .get_vocab() method - vocab size: {len(vocab)}[/blue]\")\n",
    "                \n",
    "            # Method 3: Fallback - try to access vocab through other methods\n",
    "            elif hasattr(tokenizer, '_tokenizer') and hasattr(tokenizer._tokenizer, 'get_vocab'):\n",
    "                vocab = tokenizer._tokenizer.get_vocab()\n",
    "                console.print(f\"[blue]Using ._tokenizer.get_vocab() - vocab size: {len(vocab)}[/blue]\")\n",
    "                \n",
    "            else:\n",
    "                # No vocab access method found - skip token addition\n",
    "                console.print(\"[yellow]No vocab access method found, skipping special token addition[/yellow]\")\n",
    "                return model, tokenizer\n",
    "            \n",
    "            # Check which tokens are new\n",
    "            new_tokens = [token for token in special_tokens if token not in vocab]\n",
    "            \n",
    "            if new_tokens:\n",
    "                # Add new tokens\n",
    "                added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "                if added_tokens > 0:\n",
    "                    model.resize_token_embeddings(len(tokenizer))\n",
    "                    console.print(f\"[yellow]Added {added_tokens} new tokens: {new_tokens}[/yellow]\")\n",
    "                else:\n",
    "                    console.print(\"[blue]Tokens already exist in vocabulary[/blue]\")\n",
    "            else:\n",
    "                console.print(\"[blue]All special tokens already in vocabulary[/blue]\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            console.print(f\"[yellow]Warning: Could not check/add vocabulary - {e}[/yellow]\")\n",
    "            console.print(\"[yellow]Continuing without special token addition[/yellow]\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå Error loading model {model_name}: {e}[/red]\")\n",
    "        raise\n",
    "\n",
    "def create_data_loaders(\n",
    "    data: Dict[str, List[Dict]],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    batch_size: int = 16,\n",
    "    max_length: int = 384,\n",
    "    num_workers: int = 0  # Set to 0 for Colab compatibility\n",
    ") -> Dict[str, DataLoader]:\n",
    "    \"\"\"Create data loaders for training\"\"\"\n",
    "    \n",
    "    data_loaders = {}\n",
    "    \n",
    "    for split, split_data in data.items():\n",
    "        dataset = ViGECDataset(\n",
    "            data=split_data,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=max_length,\n",
    "            is_train=(split == 'train')\n",
    "        )\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=(split == 'train'),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            drop_last=(split == 'train')\n",
    "        )\n",
    "        \n",
    "        data_loaders[split] = data_loader\n",
    "    \n",
    "    return data_loaders\n",
    "\n",
    "def create_contrastive_data_loaders(\n",
    "    data_dir: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    batch_size: int = 8,\n",
    "    max_length: int = 384,\n",
    "    num_workers: int = 0\n",
    ") -> Dict[str, DataLoader]:\n",
    "    \"\"\"Create data loaders for contrastive learning\"\"\"\n",
    "    \n",
    "    data_loaders = {}\n",
    "    \n",
    "    for split in ['train', 'validation']:\n",
    "        file_path = os.path.join(data_dir, f\"{split}_contrastive.json\")\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            # Load contrastive data\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                split_data = json.load(f)\n",
    "            \n",
    "            dataset = ContrastiveDataset(\n",
    "                data=split_data,\n",
    "                tokenizer=tokenizer,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            data_loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=(split == 'train'),\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                drop_last=(split == 'train')\n",
    "            )\n",
    "            \n",
    "            data_loaders[split] = data_loader\n",
    "            console.print(f\"[green]Created {split} contrastive dataloader: {len(dataset)} samples[/green]\")\n",
    "    \n",
    "    return data_loaders\n",
    "\n",
    "def save_processed_data(data: Dict[str, List[Dict]], output_dir: str):\n",
    "    \"\"\"Save processed data to disk\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for split, split_data in data.items():\n",
    "        output_path = os.path.join(output_dir, f\"{split}.json\")\n",
    "        \n",
    "        df = pd.DataFrame(split_data)\n",
    "        df.to_json(output_path, orient='records', force_ascii=False, indent=2)\n",
    "        \n",
    "        console.print(f\"[green]Saved {split} data to {output_path}[/green]\")\n",
    "\n",
    "def load_processed_data(data_dir: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Load processed data from disk\"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        file_path = os.path.join(data_dir, f\"{split}.json\")\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_json(file_path, orient='records')\n",
    "            data[split] = df.to_dict('records')\n",
    "            console.print(f\"[green]Loaded {split}: {len(data[split])} samples[/green]\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def test_tokenizer_compatibility(model_name: str) -> bool:\n",
    "    \"\"\"Test tokenizer compatibility and vocabulary access\"\"\"\n",
    "    \n",
    "    console.print(f\"[bold]üß™ Testing compatibility for: {model_name}[/bold]\")\n",
    "    \n",
    "    try:\n",
    "        model, tokenizer = get_model_and_tokenizer(model_name)\n",
    "        \n",
    "        # Test basic tokenization\n",
    "        test_text = \"T√¥i ƒëi h·ªçc tr∆∞·ªùng ƒë·∫°i h·ªçc.\"\n",
    "        if hasattr(tokenizer, 'task_prefix'):\n",
    "            test_text = tokenizer.task_prefix + test_text\n",
    "        \n",
    "        tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "        console.print(f\"[green]‚úÖ Tokenization successful - shape: {tokens['input_ids'].shape}[/green]\")\n",
    "        \n",
    "        # Test vocabulary access methods\n",
    "        vocab_methods = []\n",
    "        if hasattr(tokenizer, 'vocab'):\n",
    "            vocab_methods.append('.vocab')\n",
    "        if hasattr(tokenizer, 'get_vocab'):\n",
    "            vocab_methods.append('.get_vocab()')\n",
    "        if hasattr(tokenizer, '_tokenizer') and hasattr(tokenizer._tokenizer, 'get_vocab'):\n",
    "            vocab_methods.append('._tokenizer.get_vocab()')\n",
    "        \n",
    "        console.print(f\"[green]‚úÖ Available vocab methods: {vocab_methods}[/green]\")\n",
    "        \n",
    "        # Test data loading\n",
    "        sample_data = [{'source': 'T√¥i ƒëi h·ªçc.', 'target': 'T√¥i ƒëi h·ªçc.'}]\n",
    "        dataset = ViGECDataset(sample_data, tokenizer, max_length=128)\n",
    "        sample = dataset[0]\n",
    "        console.print(f\"[green]‚úÖ Dataset creation successful[/green]\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå Error testing {model_name}: {e}[/red]\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def verify_bartpho_fix() -> bool:\n",
    "    \"\"\"Verify that the BARTpho fix is working\"\"\"\n",
    "    \n",
    "    console.print(\"[bold green]üß™ Verifying BARTpho Fix...[/bold green]\")\n",
    "    \n",
    "    models_to_test = [\n",
    "        \"vinai/bartpho-syllable\",\n",
    "        # \"VietAI/vit5-base\"  # Uncomment to test ViT5 as well\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        console.print(f\"\\\\n{'='*50}\")\n",
    "        console.print(f\"Testing: {model_name}\")\n",
    "        console.print(f\"{'='*50}\")\n",
    "        \n",
    "        success = test_tokenizer_compatibility(model_name)\n",
    "        results[model_name] = success\n",
    "        \n",
    "        if success:\n",
    "            console.print(f\"[green]‚úÖ {model_name}: PASSED[/green]\")\n",
    "        else:\n",
    "            console.print(f\"[red]‚ùå {model_name}: FAILED[/red]\")\n",
    "    \n",
    "    # Summary\n",
    "    console.print(f\"\\\\n{'='*60}\")\n",
    "    console.print(\"[bold]üéØ Test Summary[/bold]\")\n",
    "    console.print(f\"{'='*60}\")\n",
    "    \n",
    "    for model_name, success in results.items():\n",
    "        status = \"‚úÖ PASSED\" if success else \"‚ùå FAILED\"\n",
    "        console.print(f\"{model_name}: {status}\")\n",
    "    \n",
    "    all_passed = all(results.values())\n",
    "    \n",
    "    if all_passed:\n",
    "        console.print(\"\\\\n[bold green]üéâ All tests passed! BARTpho fix is working correctly.[/bold green]\")\n",
    "    else:\n",
    "        console.print(\"\\\\n[bold red]‚ö†Ô∏è Some tests failed. Check the errors above.[/bold red]\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "def check_system_requirements() -> Dict[str, bool]:\n",
    "    \"\"\"Check system requirements for training\"\"\"\n",
    "    \n",
    "    console.print(\"[bold blue]üîç Checking System Requirements[/bold blue]\")\n",
    "    \n",
    "    checks = {}\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    checks['cuda'] = cuda_available\n",
    "    console.print(f\"GPU Available: {'‚úÖ' if cuda_available else '‚ùå'}\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        console.print(f\"GPU: {gpu_name}\")\n",
    "        console.print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        checks['gpu_memory'] = gpu_memory\n",
    "    else:\n",
    "        checks['gpu_memory'] = 0\n",
    "    \n",
    "    # Check disk space\n",
    "    import shutil\n",
    "    disk_space = shutil.disk_usage('.').free / 1e9\n",
    "    checks['disk_space'] = disk_space\n",
    "    console.print(f\"Available Disk Space: {disk_space:.1f} GB\")\n",
    "    \n",
    "    # Check RAM\n",
    "    try:\n",
    "        import psutil\n",
    "        ram = psutil.virtual_memory().total / 1e9\n",
    "        checks['ram'] = ram\n",
    "        console.print(f\"Total RAM: {ram:.1f} GB\")\n",
    "    except ImportError:\n",
    "        checks['ram'] = 0\n",
    "        console.print(\"RAM: Unable to check\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    ready = (\n",
    "        checks['cuda'] and \n",
    "        checks['gpu_memory'] >= 6 and \n",
    "        checks['disk_space'] >= 5\n",
    "    )\n",
    "    \n",
    "    console.print(f\"\\\\n[bold]System Ready for Training: {'‚úÖ' if ready else '‚ùå'}[/bold]\")\n",
    "    \n",
    "    if not ready:\n",
    "        console.print(\"[yellow]‚ö†Ô∏è Recommendations:[/yellow]\")\n",
    "        if not checks['cuda']:\n",
    "            console.print(\"  - Enable GPU runtime in Colab\")\n",
    "        if checks['gpu_memory'] < 6:\n",
    "            console.print(\"  - Use smaller batch sizes\")\n",
    "        if checks['disk_space'] < 5:\n",
    "            console.print(\"  - Free up disk space\")\n",
    "    \n",
    "    return checks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run verification when script is executed directly\n",
    "    verify_bartpho_fix()\n",
    "'''\n",
    "\n",
    "# Write the complete file\n",
    "with open('data_utils.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(data_utils_complete)\n",
    "\n",
    "print(\"‚úÖ Created COMPLETE data_utils.py with all functions\")\n",
    "\n",
    "# Test the fix immediately\n",
    "try:\n",
    "    from data_utils import verify_bartpho_fix, check_system_requirements\n",
    "    \n",
    "    print(\"\\nüîç Running system checks...\")\n",
    "    system_checks = check_system_requirements()\n",
    "    \n",
    "    print(\"\\nüß™ Testing BARTpho tokenizer fix...\")\n",
    "    fix_success = verify_bartpho_fix()\n",
    "    \n",
    "    if fix_success:\n",
    "        print(\"\\nüéâ BARTpho tokenizer fix is working correctly!\")\n",
    "        print(\"‚úÖ Ready to proceed with training\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå BARTpho test failed - please check the errors above\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ Setup Status: ‚úÖ COMPLETE - Ready for Vietnamese GEC Training\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (if needed)\n",
    "# !git clone https://github.com/your-repo/CL_GEC.git\n",
    "# %cd CL_GEC\n",
    "\n",
    "# Or upload files directly to Colab\n",
    "import os\n",
    "import json\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Create directory structure\n",
    "directories = [\n",
    "    './models/base',\n",
    "    './models/contrastive', \n",
    "    './data/processed',\n",
    "    './data/contrastive',\n",
    "    './evaluation_results',\n",
    "    './logs'\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    console.print(f\"‚úÖ Created: {directory}\")\n",
    "\n",
    "# Create configuration file\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"name\": \"vinai/bartpho-syllable\",  # Change to \"VietAI/vit5-base\" for ViT5\n",
    "        \"max_length\": 384,\n",
    "        \"batch_size\": 8,  # Will be adjusted based on GPU memory\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"num_epochs\": 3\n",
    "    },\n",
    "    \"contrastive\": {\n",
    "        \"lambda_cl\": 1.0,\n",
    "        \"temperature\": 0.25,\n",
    "        \"rdrop_alpha\": 4.0,\n",
    "        \"cl_epochs\": 2\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"use_contrastive_search\": True,\n",
    "        \"contrastive_alpha\": 0.7,\n",
    "        \"contrastive_k\": 5,\n",
    "        \"num_beams\": 5\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"label_smoothing\": 0.1,\n",
    "        \"mixed_precision\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "with open('config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "console.print(\"üìÅ Project structure created successfully!\")\n",
    "console.print(\"‚öôÔ∏è Configuration saved to config.json\")\n",
    "\n",
    "# Display configuration\n",
    "console.print(\"\\nüìã [bold]Current Configuration:[/bold]\")\n",
    "for section, settings in config.items():\n",
    "    console.print(f\"\\n  [yellow]{section.upper()}:[/yellow]\")\n",
    "    for key, value in settings.items():\n",
    "        console.print(f\"    {key}: {value}\")\n",
    "\n",
    "console.print(\"\\nüí° [bold]Tip:[/bold] You can modify config.json to adjust parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f33240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload all Python files to Colab\n",
    "# Use the file upload button in Colab to upload:\n",
    "# - data_utils.py\n",
    "# - negative_sampler.py\n",
    "# - contrastive_trainer.py\n",
    "# - inference.py\n",
    "# - evaluator.py\n",
    "# - evaluate_model.py\n",
    "\n",
    "# Verify files are uploaded\n",
    "required_files = [\n",
    "    'data_utils.py', 'negative_sampler.py',\n",
    "    'contrastive_trainer.py', 'inference.py', 'evaluator.py', 'evaluate_model.py'\n",
    "]\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ {file} found\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} missing - please upload this file\")\n",
    "\n",
    "# Create all necessary training files directly in the notebook\n",
    "# This eliminates the need for file uploads\n",
    "\n",
    "console.print(\"üìù Creating training modules...\")\n",
    "\n",
    "# 1. Create Simple Base Trainer\n",
    "base_trainer_code = '''\"\"\"\n",
    "Simple Base Trainer for Vietnamese GEC - Colab Optimized\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from rich.console import Console\n",
    "import logging\n",
    "\n",
    "console = Console()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SimpleBaseTrainer:\n",
    "    \"\"\"Simple trainer for base model fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = 'config.json'):\n",
    "        # Load configuration\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        console.print(f\"[blue]Using device: {self.device}[/blue]\")\n",
    "        \n",
    "        # Adjust batch size based on GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            if gpu_memory < 8:\n",
    "                self.config['model']['batch_size'] = 4\n",
    "                self.config['training']['gradient_accumulation_steps'] = 8\n",
    "            elif gpu_memory < 16:\n",
    "                self.config['model']['batch_size'] = 8\n",
    "                self.config['training']['gradient_accumulation_steps'] = 4\n",
    "            \n",
    "            console.print(f\"[yellow]Adjusted batch_size to {self.config['model']['batch_size']} for {gpu_memory:.1f}GB GPU[/yellow]\")\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load processed data\"\"\"\n",
    "        from data_utils import load_processed_data\n",
    "        \n",
    "        console.print(\"[blue]üì• Loading processed data...[/blue]\")\n",
    "        data = load_processed_data(\"./data/processed\")\n",
    "        \n",
    "        if not data:\n",
    "            raise ValueError(\"No data found in ./data/processed\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def prepare_model_and_tokenizer(self):\n",
    "        \"\"\"Load model and tokenizer\"\"\"\n",
    "        from data_utils import get_model_and_tokenizer\n",
    "        \n",
    "        model_name = self.config['model']['name']\n",
    "        console.print(f\"[blue]ü§ñ Loading model: {model_name}[/blue]\")\n",
    "        \n",
    "        model, tokenizer = get_model_and_tokenizer(model_name)\n",
    "        model.to(self.device)\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the base model\"\"\"\n",
    "        console.print(\"[bold green]üöÄ Starting base model training...[/bold green]\")\n",
    "        \n",
    "        # Load data and model\n",
    "        data = self.load_data()\n",
    "        model, tokenizer = self.prepare_model_and_tokenizer()\n",
    "        \n",
    "        # Create datasets\n",
    "        from data_utils import create_data_loaders\n",
    "        data_loaders = create_data_loaders(\n",
    "            data,\n",
    "            tokenizer,\n",
    "            batch_size=self.config['model']['batch_size'],\n",
    "            max_length=self.config['model']['max_length']\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=self.config['model']['learning_rate'],\n",
    "            weight_decay=self.config['training']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Setup scheduler\n",
    "        total_steps = len(data_loaders['train']) * self.config['model']['num_epochs']\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.config['training']['warmup_steps'],\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(self.config['model']['num_epochs']):\n",
    "            console.print(f\"[blue]üìö Epoch {epoch + 1}/{self.config['model']['num_epochs']}[/blue]\")\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch in tqdm(data_loaders['train'], desc=\"Training\"):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update weights\n",
    "                if (num_batches + 1) % self.config['training']['gradient_accumulation_steps'] == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            console.print(f\"[green]üìä Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}[/green]\")\n",
    "            \n",
    "            # Validation\n",
    "            if 'validation' in data_loaders:\n",
    "                val_loss = self._validate(model, data_loaders['validation'])\n",
    "                console.print(f\"[yellow]üìä Validation Loss: {val_loss:.4f}[/yellow]\")\n",
    "        \n",
    "        # Save model\n",
    "        output_dir = \"./models/base/final\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        console.print(f\"[green]‚úÖ Model saved to {output_dir}[/green]\")\n",
    "        console.print(\"[bold green]‚úÖ Base model training completed![/bold green]\")\n",
    "        \n",
    "        return output_dir\n",
    "    \n",
    "    def _validate(self, model, val_loader):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                total_loss += outputs.loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        model.train()\n",
    "        return total_loss / num_batches\n",
    "\n",
    "def train_base_model():\n",
    "    \"\"\"Convenience function to train base model\"\"\"\n",
    "    trainer = SimpleBaseTrainer()\n",
    "    return trainer.train()\n",
    "'''\n",
    "\n",
    "# 2. Create Simple Negative Sampler\n",
    "negative_sampler_code = '''\"\"\"\n",
    "Simple Negative Sample Generator for Contrastive Learning\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from rich.console import Console\n",
    "from data_utils import get_model_and_tokenizer\n",
    "\n",
    "console = Console()\n",
    "\n",
    "class SimpleNegativeSampler:\n",
    "    \"\"\"Generate negative samples for contrastive learning\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        console.print(f\"[blue]üé≠ Loading model from {model_path}[/blue]\")\n",
    "        self.model, self.tokenizer = get_model_and_tokenizer(model_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Check if this is ViT5\n",
    "        self.use_prefix = hasattr(self.tokenizer, 'task_prefix')\n",
    "    \n",
    "    def generate_negatives(self, source_text: str, target_text: str, num_negatives: int = 3):\n",
    "        \"\"\"Generate negative samples for one example\"\"\"\n",
    "        \n",
    "        # Add prefix for ViT5\n",
    "        input_text = source_text\n",
    "        if self.use_prefix:\n",
    "            input_text = self.tokenizer.task_prefix + source_text\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=384,\n",
    "            truncation=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate with beam search to get diverse candidates\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                num_beams=6,\n",
    "                num_return_sequences=6,\n",
    "                max_length=384,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                length_penalty=1.0\n",
    "            )\n",
    "        \n",
    "        # Decode outputs\n",
    "        candidates = []\n",
    "        for output in outputs:\n",
    "            decoded = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "            \n",
    "            # Remove prefix if present\n",
    "            if self.use_prefix and decoded.startswith(self.tokenizer.task_prefix):\n",
    "                decoded = decoded[len(self.tokenizer.task_prefix):].strip()\n",
    "            \n",
    "            candidates.append(decoded)\n",
    "        \n",
    "        # Filter negatives (different from target)\n",
    "        negatives = []\n",
    "        for candidate in candidates:\n",
    "            if candidate != target_text and candidate != source_text and candidate.strip():\n",
    "                negatives.append(candidate)\n",
    "                if len(negatives) >= num_negatives:\n",
    "                    break\n",
    "        \n",
    "        # Add source as negative if it contains errors\n",
    "        if source_text != target_text and len(negatives) < num_negatives:\n",
    "            negatives.append(source_text)\n",
    "        \n",
    "        # Pad with variations if we don't have enough\n",
    "        while len(negatives) < num_negatives:\n",
    "            if negatives:\n",
    "                negatives.append(negatives[0])  # Duplicate first negative\n",
    "            else:\n",
    "                negatives.append(source_text)  # Fallback\n",
    "        \n",
    "        return negatives[:num_negatives]\n",
    "    \n",
    "    def generate_contrastive_dataset(self, data, output_path: str, batch_size: int = 4):\n",
    "        \"\"\"Generate contrastive dataset\"\"\"\n",
    "        \n",
    "        console.print(f\"[blue]üé≠ Generating negative samples for {len(data)} examples...[/blue]\")\n",
    "        \n",
    "        contrastive_data = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(data), batch_size), desc=\"Generating negatives\"):\n",
    "            batch = data[i:i + batch_size]\n",
    "            \n",
    "            for item in batch:\n",
    "                source = item['source']\n",
    "                target = item['target']\n",
    "                \n",
    "                # Generate negatives\n",
    "                negatives = self.generate_negatives(source, target)\n",
    "                \n",
    "                # Create contrastive example\n",
    "                contrastive_item = {\n",
    "                    'source': source,\n",
    "                    'target': target,\n",
    "                    'positive': target,  # For compatibility\n",
    "                    'negatives': negatives\n",
    "                }\n",
    "                contrastive_data.append(contrastive_item)\n",
    "        \n",
    "        # Save to file\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(contrastive_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        console.print(f\"[green]üíæ Saved {len(contrastive_data)} contrastive examples to {output_path}[/green]\")\n",
    "        return contrastive_data\n",
    "\n",
    "def generate_negative_samples():\n",
    "    \"\"\"Convenience function to generate negative samples\"\"\"\n",
    "    from data_utils import load_processed_data\n",
    "    \n",
    "    # Load data\n",
    "    data = load_processed_data(\"./data/processed\")\n",
    "    \n",
    "    # Create sampler\n",
    "    sampler = SimpleNegativeSampler(\"./models/base/final\")\n",
    "    \n",
    "    # Generate for train and validation splits\n",
    "    for split in ['train', 'validation']:\n",
    "        if split in data:\n",
    "            console.print(f\"[yellow]Processing {split} split...[/yellow]\")\n",
    "            \n",
    "            output_path = f\"./data/contrastive/{split}_contrastive.json\"\n",
    "            sampler.generate_contrastive_dataset(\n",
    "                data[split],\n",
    "                output_path,\n",
    "                batch_size=2  # Small batch for memory efficiency\n",
    "            )\n",
    "    \n",
    "    console.print(\"[green]‚úÖ Negative sample generation completed![/green]\")\n",
    "'''\n",
    "\n",
    "# 3. Create Simple Contrastive Trainer\n",
    "contrastive_trainer_code = '''\"\"\"\n",
    "Simple Contrastive Learning Trainer\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from rich.console import Console\n",
    "from data_utils import get_model_and_tokenizer, create_contrastive_data_loaders\n",
    "\n",
    "console = Console()\n",
    "\n",
    "class SimpleContrastiveTrainer:\n",
    "    \"\"\"Simple contrastive learning trainer\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = 'config.json'):\n",
    "        # Load configuration\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.lambda_cl = self.config['contrastive']['lambda_cl']\n",
    "        self.temperature = self.config['contrastive']['temperature']\n",
    "    \n",
    "    def contrastive_loss(self, source_hidden, positive_hidden, negative_hidden):\n",
    "        \"\"\"Compute contrastive loss\"\"\"\n",
    "        \n",
    "        # Normalize representations\n",
    "        source_hidden = F.normalize(source_hidden, dim=-1)\n",
    "        positive_hidden = F.normalize(positive_hidden, dim=-1)\n",
    "        negative_hidden = F.normalize(negative_hidden, dim=-1)  # [batch, num_neg, hidden]\n",
    "        \n",
    "        # Positive similarity\n",
    "        pos_sim = torch.sum(source_hidden * positive_hidden, dim=-1) / self.temperature  # [batch]\n",
    "        \n",
    "        # Negative similarities\n",
    "        neg_sim = torch.bmm(\n",
    "            negative_hidden, \n",
    "            source_hidden.unsqueeze(-1)\n",
    "        ).squeeze(-1) / self.temperature  # [batch, num_neg]\n",
    "        \n",
    "        # Contrastive loss\n",
    "        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)  # [batch, 1 + num_neg]\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train with contrastive learning\"\"\"\n",
    "        console.print(\"[bold blue]üîÑ Starting contrastive learning training...[/bold blue]\")\n",
    "        \n",
    "        # Load model\n",
    "        model, tokenizer = get_model_and_tokenizer(\"./models/base/final\")\n",
    "        model.to(self.device)\n",
    "        \n",
    "        # Create contrastive data loaders\n",
    "        data_loaders = create_contrastive_data_loaders(\n",
    "            \"./data/contrastive\",\n",
    "            tokenizer,\n",
    "            batch_size=self.config['model']['batch_size'] // 2,  # Smaller batch for contrastive\n",
    "            max_length=self.config['model']['max_length']\n",
    "        )\n",
    "        \n",
    "        if not data_loaders:\n",
    "            raise ValueError(\"No contrastive data found!\")\n",
    "        \n",
    "        # Setup optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=self.config['model']['learning_rate'] / 2,  # Lower LR for contrastive\n",
    "            weight_decay=self.config['training']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(self.config['contrastive']['cl_epochs']):\n",
    "            console.print(f\"[blue]üîÑ Contrastive Epoch {epoch + 1}/{self.config['contrastive']['cl_epochs']}[/blue]\")\n",
    "            \n",
    "            epoch_ce_loss = 0\n",
    "            epoch_cl_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch in tqdm(data_loaders['train'], desc=\"Contrastive Training\"):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass for source\n",
    "                source_outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['positive_ids']\n",
    "                )\n",
    "                \n",
    "                # Cross-entropy loss\n",
    "                ce_loss = source_outputs.loss\n",
    "                \n",
    "                # Get encoder hidden states for contrastive loss\n",
    "                source_hidden = source_outputs.encoder_last_hidden_state.mean(dim=1)  # [batch, hidden]\n",
    "                \n",
    "                # Forward pass for positive\n",
    "                positive_outputs = model.encoder(\n",
    "                    input_ids=batch['positive_ids'],\n",
    "                    attention_mask=batch['positive_attention_mask']\n",
    "                )\n",
    "                positive_hidden = positive_outputs.last_hidden_state.mean(dim=1)  # [batch, hidden]\n",
    "                \n",
    "                # Forward pass for negatives\n",
    "                batch_size, num_neg, seq_len = batch['negative_ids'].shape\n",
    "                negative_ids = batch['negative_ids'].view(batch_size * num_neg, seq_len)\n",
    "                negative_mask = batch['negative_attention_mask'].view(batch_size * num_neg, seq_len)\n",
    "                \n",
    "                negative_outputs = model.encoder(\n",
    "                    input_ids=negative_ids,\n",
    "                    attention_mask=negative_mask\n",
    "                )\n",
    "                negative_hidden = negative_outputs.last_hidden_state.mean(dim=1)  # [batch*num_neg, hidden]\n",
    "                negative_hidden = negative_hidden.view(batch_size, num_neg, -1)  # [batch, num_neg, hidden]\n",
    "                \n",
    "                # Contrastive loss\n",
    "                cl_loss = self.contrastive_loss(source_hidden, positive_hidden, negative_hidden)\n",
    "                \n",
    "                # Combined loss\n",
    "                total_loss = ce_loss + self.lambda_cl * cl_loss\n",
    "                \n",
    "                # Backward pass\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                epoch_ce_loss += ce_loss.item()\n",
    "                epoch_cl_loss += cl_loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_ce_loss = epoch_ce_loss / num_batches\n",
    "            avg_cl_loss = epoch_cl_loss / num_batches\n",
    "            \n",
    "            console.print(f\"[green]üìä Epoch {epoch + 1} - CE Loss: {avg_ce_loss:.4f}, CL Loss: {avg_cl_loss:.4f}[/green]\")\n",
    "        \n",
    "        # Save model\n",
    "        output_dir = \"./models/contrastive/final\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        console.print(f\"[green]‚úÖ Contrastive model saved to {output_dir}[/green]\")\n",
    "        console.print(\"[bold green]‚úÖ Contrastive learning training completed![/bold green]\")\n",
    "        \n",
    "        return output_dir\n",
    "\n",
    "def train_contrastive_model():\n",
    "    \"\"\"Convenience function to train contrastive model\"\"\"\n",
    "    trainer = SimpleContrastiveTrainer()\n",
    "    return trainer.train()\n",
    "'''\n",
    "\n",
    "# 4. Create Simple Inference\n",
    "inference_code = '''\"\"\"\n",
    "Simple Inference for Vietnamese GEC\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from rich.console import Console\n",
    "from data_utils import get_model_and_tokenizer\n",
    "\n",
    "console = Console()\n",
    "\n",
    "class SimpleInference:\n",
    "    \"\"\"Simple inference for Vietnamese GEC\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, config_path: str = 'config.json'):\n",
    "        # Load configuration\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load model\n",
    "        console.print(f\"[blue]üîÆ Loading model from {model_path}[/blue]\")\n",
    "        self.model, self.tokenizer = get_model_and_tokenizer(model_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Check if this is ViT5\n",
    "        self.use_prefix = hasattr(self.tokenizer, 'task_prefix')\n",
    "        \n",
    "        # Inference settings\n",
    "        self.use_contrastive = self.config['inference']['use_contrastive_search']\n",
    "        self.alpha = self.config['inference']['contrastive_alpha']\n",
    "        self.k = self.config['inference']['contrastive_k']\n",
    "        self.num_beams = self.config['inference']['num_beams']\n",
    "    \n",
    "    def correct_text(self, text: str) -> str:\n",
    "        \"\"\"Correct a single text\"\"\"\n",
    "        \n",
    "        # Add prefix for ViT5\n",
    "        input_text = text\n",
    "        if self.use_prefix:\n",
    "            input_text = self.tokenizer.task_prefix + text\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=384,\n",
    "            truncation=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            if self.use_contrastive:\n",
    "                # Contrastive search\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    penalty_alpha=self.alpha,\n",
    "                    top_k=self.k,\n",
    "                    max_length=384,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            else:\n",
    "                # Beam search\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    num_beams=self.num_beams,\n",
    "                    max_length=384,\n",
    "                    do_sample=False,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "        \n",
    "        # Decode\n",
    "        corrected = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Remove prefix if present\n",
    "        if self.use_prefix and corrected.startswith(self.tokenizer.task_prefix):\n",
    "            corrected = corrected[len(self.tokenizer.task_prefix):].strip()\n",
    "        \n",
    "        return corrected\n",
    "    \n",
    "    def correct_batch(self, texts: list) -> list:\n",
    "        \"\"\"Correct a batch of texts\"\"\"\n",
    "        return [self.correct_text(text) for text in texts]\n",
    "\n",
    "def create_inference_engine(model_path: str = \"./models/contrastive/final\"):\n",
    "    \"\"\"Create inference engine\"\"\"\n",
    "    return SimpleInference(model_path)\n",
    "'''\n",
    "\n",
    "# Write all files\n",
    "files_to_create = {\n",
    "    'base_trainer.py': base_trainer_code,\n",
    "    'negative_sampler.py': negative_sampler_code,\n",
    "    'contrastive_trainer.py': contrastive_trainer_code,\n",
    "    'inference.py': inference_code\n",
    "}\n",
    "\n",
    "for filename, code in files_to_create.items():\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(code)\n",
    "    console.print(f\"‚úÖ Created {filename}\")\n",
    "\n",
    "console.print(\"\\nüéØ All training modules created successfully!\")\n",
    "console.print(\"üìã Available modules:\")\n",
    "console.print(\"  üéØ base_trainer.py - Base model training\")\n",
    "console.print(\"  üé≠ negative_sampler.py - Negative sample generation\")\n",
    "console.print(\"  üîÑ contrastive_trainer.py - Contrastive learning\")\n",
    "console.print(\"  üîÆ inference.py - Text correction inference\")\n",
    "console.print(\"\\n‚úÖ Ready for training pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e002faf",
   "metadata": {},
   "source": [
    "## üìä Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and verify environment\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from rich.console import Console\n",
    "from data_utils import load_vigec_dataset, save_processed_data, get_model_and_tokenizer, check_system_requirements\n",
    "import json\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Check versions for compatibility\n",
    "console.print(\"[bold blue]üîç Environment Check[/bold blue]\")\n",
    "console.print(f\"Python: {sys.version}\")\n",
    "console.print(f\"PyTorch: {torch.__version__}\")\n",
    "console.print(f\"NumPy: {np.__version__}\")\n",
    "\n",
    "# Verify numpy version is compatible\n",
    "if np.__version__.startswith('2.'):\n",
    "    console.print(\"[red]‚ö†Ô∏è WARNING: NumPy 2.0 detected - this may cause wandb issues[/red]\")\n",
    "    console.print(\"[yellow]Training will continue without wandb if conflicts occur[/yellow]\")\n",
    "else:\n",
    "    console.print(\"[green]‚úÖ NumPy version is compatible[/green]\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "console.print(f\"üî• Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    console.print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    console.print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Optimize settings based on GPU memory\n",
    "    if gpu_memory < 8:\n",
    "        console.print(\"[yellow]‚ö†Ô∏è Limited GPU memory - using smaller batch sizes[/yellow]\")\n",
    "        BATCH_SIZE = 4\n",
    "        GRADIENT_ACCUMULATION = 8\n",
    "    elif gpu_memory < 16:\n",
    "        console.print(\"[blue]üîÑ Medium GPU memory - using moderate batch sizes[/blue]\")\n",
    "        BATCH_SIZE = 8\n",
    "        GRADIENT_ACCUMULATION = 4\n",
    "    else:\n",
    "        console.print(\"[green]‚úÖ High GPU memory - using optimal batch sizes[/green]\")\n",
    "        BATCH_SIZE = 16\n",
    "        GRADIENT_ACCUMULATION = 2\n",
    "else:\n",
    "    console.print(\"[red]‚ùå No GPU available - training will be very slow[/red]\")\n",
    "    BATCH_SIZE = 2\n",
    "    GRADIENT_ACCUMULATION = 16\n",
    "\n",
    "console.print(f\"üìä Optimized settings: batch_size={BATCH_SIZE}, grad_accumulation={GRADIENT_ACCUMULATION}\")\n",
    "\n",
    "# Update configuration\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config['model']['batch_size'] = BATCH_SIZE\n",
    "config['training']['gradient_accumulation_steps'] = GRADIENT_ACCUMULATION\n",
    "\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "console.print(f\"[green]‚öôÔ∏è Updated config: batch_size={BATCH_SIZE}, grad_accumulation={GRADIENT_ACCUMULATION}[/green]\")\n",
    "\n",
    "# Test wandb availability\n",
    "try:\n",
    "    import wandb\n",
    "    console.print(\"[green]‚úÖ Wandb available for experiment tracking[/green]\")\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    console.print(f\"[yellow]‚ö†Ô∏è Wandb not available: {e}[/yellow]\")\n",
    "    console.print(\"[yellow]Training will continue without experiment tracking[/yellow]\")\n",
    "    WANDB_AVAILABLE = False\n",
    "\n",
    "# Data Preparation - Load and preprocess viGEC dataset\n",
    "\n",
    "# Check system requirements first\n",
    "console.print(\"[bold blue]üîç Checking System Requirements[/bold blue]\")\n",
    "system_checks = check_system_requirements()\n",
    "\n",
    "# Optimize settings based on available resources\n",
    "if system_checks['gpu_memory'] < 8:\n",
    "    BATCH_SIZE = 4\n",
    "    GRAD_ACCUMULATION = 8\n",
    "    console.print(\"[yellow]‚ö†Ô∏è Using smaller batch size for limited GPU memory[/yellow]\")\n",
    "elif system_checks['gpu_memory'] < 16:\n",
    "    BATCH_SIZE = 8  \n",
    "    GRAD_ACCUMULATION = 4\n",
    "    console.print(\"[blue]üìä Using moderate batch size[/blue]\")\n",
    "else:\n",
    "    BATCH_SIZE = 16\n",
    "    GRAD_ACCUMULATION = 2\n",
    "    console.print(\"[green]‚úÖ Using optimal batch size[/green]\")\n",
    "\n",
    "# Update configuration\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config['model']['batch_size'] = BATCH_SIZE\n",
    "config['training']['gradient_accumulation_steps'] = GRAD_ACCUMULATION\n",
    "\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "console.print(f\"[green]‚öôÔ∏è Updated config: batch_size={BATCH_SIZE}, grad_accumulation={GRAD_ACCUMULATION}[/green]\")\n",
    "\n",
    "# Load and preprocess dataset\n",
    "console.print(\"\\n[bold blue]üì• Loading viGEC Dataset[/bold blue]\")\n",
    "\n",
    "try:\n",
    "    # Load the dataset with small test subset for faster evaluation\n",
    "    data = load_vigec_dataset(\n",
    "        dataset_name=\"phuhuy-se1/viGEC\",\n",
    "        test_subset_ratio=0.05  # Use 5% of test set for faster evaluation\n",
    "    )\n",
    "    \n",
    "    # Save processed data\n",
    "    save_processed_data(data, \"./data/processed\")\n",
    "    \n",
    "    console.print(\"[bold green]‚úÖ Data preprocessing completed![/bold green]\")\n",
    "    \n",
    "    # Display statistics\n",
    "    console.print(\"\\n[bold]üìä Dataset Statistics:[/bold]\")\n",
    "    total_samples = 0\n",
    "    for split, split_data in data.items():\n",
    "        console.print(f\"  {split}: {len(split_data):,} samples\")\n",
    "        total_samples += len(split_data)\n",
    "    \n",
    "    console.print(f\"  [bold]Total: {total_samples:,} samples[/bold]\")\n",
    "    \n",
    "    # Show sample data\n",
    "    if 'train' in data and len(data['train']) > 0:\n",
    "        console.print(\"\\n[bold]üìù Sample Data:[/bold]\")\n",
    "        sample = data['train'][0]\n",
    "        console.print(f\"  Source: {sample['source']}\")\n",
    "        console.print(f\"  Target: {sample['target']}\")\n",
    "    \n",
    "    console.print(\"\\n[green]üéØ Ready for model training![/green]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    console.print(f\"[red]‚ùå Error loading dataset: {e}[/red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Login to Wandb for experiment tracking\n",
    "# Uncomment the lines below if you want to use Wandb for tracking\n",
    "\n",
    "# !wandb login\n",
    "\n",
    "# Or login programmatically:\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "\n",
    "console.print(\"üìà [bold]Wandb Setup (Optional):[/bold]\")\n",
    "console.print(\"  üî∏ Uncomment the lines above to enable experiment tracking\")\n",
    "console.print(\"  üî∏ Visit https://wandb.ai to get your API key\")\n",
    "console.print(\"  üî∏ Training will work without Wandb\")\n",
    "\n",
    "# Test wandb availability\n",
    "try:\n",
    "    import wandb\n",
    "    console.print(\"[green]‚úÖ Wandb is available (not logged in)[/green]\")\n",
    "except ImportError:\n",
    "    console.print(\"[yellow]‚ö†Ô∏è Wandb not installed - training will continue without tracking[/yellow]\")\n",
    "\n",
    "console.print(\"[blue]üöÄ Ready to proceed with or without experiment tracking[/blue]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76fffde",
   "metadata": {},
   "source": [
    "## üéØ Step 2: Base Model Training with Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection and Configuration\n",
    "import json\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Available models for Vietnamese GEC\n",
    "AVAILABLE_MODELS = {\n",
    "    \"vinai/bartpho-syllable\": \"BARTpho (syllable-level) - Recommended for Vietnamese\",\n",
    "    \"vinai/bartpho-word\": \"BARTpho (word-level) - Alternative option\",\n",
    "    \"VietAI/vit5-base\": \"ViT5 Base - T5-based model for Vietnamese\",\n",
    "    \"VietAI/vit5-large\": \"ViT5 Large - Larger model (requires more memory)\"\n",
    "}\n",
    "\n",
    "# Choose your model here\n",
    "MODEL_NAME = \"vinai/bartpho-syllable\"  # Change this to experiment with different models\n",
    "\n",
    "console.print(\"[bold blue]ü§ñ Model Selection[/bold blue]\")\n",
    "console.print(f\"[green]Selected Model: {MODEL_NAME}[/green]\")\n",
    "console.print(f\"[yellow]Description: {AVAILABLE_MODELS[MODEL_NAME]}[/yellow]\")\n",
    "\n",
    "console.print(\"\\n[bold]üìã Available Models:[/bold]\")\n",
    "for model, description in AVAILABLE_MODELS.items():\n",
    "    marker = \"‚úÖ\" if model == MODEL_NAME else \"  \"\n",
    "    console.print(f\"{marker} {model}: {description}\")\n",
    "\n",
    "# Update configuration with selected model\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config['model']['name'] = MODEL_NAME\n",
    "\n",
    "# Adjust settings based on model type\n",
    "if 'vit5-large' in MODEL_NAME:\n",
    "    config['model']['batch_size'] = max(2, config['model']['batch_size'] // 2)\n",
    "    config['training']['gradient_accumulation_steps'] *= 2\n",
    "    console.print(\"[yellow]‚ö†Ô∏è Adjusted batch size for large model[/yellow]\")\n",
    "\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "console.print(f\"\\n[green]‚öôÔ∏è Configuration updated with {MODEL_NAME}[/green]\")\n",
    "console.print(f\"[blue]üìä Batch size: {config['model']['batch_size']}[/blue]\")\n",
    "console.print(f\"[blue]üìä Learning rate: {config['model']['learning_rate']}[/blue]\")\n",
    "console.print(f\"[blue]üìä Max length: {config['model']['max_length']}[/blue]\")\n",
    "\n",
    "# Test model loading\n",
    "console.print(f\"\\n[yellow]üß™ Testing model loading...[/yellow]\")\n",
    "try:\n",
    "    from data_utils import test_tokenizer_compatibility\n",
    "    success = test_tokenizer_compatibility(MODEL_NAME)\n",
    "    if success:\n",
    "        console.print(\"[green]‚úÖ Model loading test passed![/green]\")\n",
    "    else:\n",
    "        console.print(\"[red]‚ùå Model loading test failed![/red]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[red]‚ùå Error testing model: {e}[/red]\")\n",
    "\n",
    "console.print(\"\\n[green]üéØ Ready for base model training![/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_trainer import train_base_model\n",
    "from rich.console import Console\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "\n",
    "console = Console()\n",
    "\n",
    "console.print(\"[bold green]üöÄ Starting Base Model Training[/bold green]\")\n",
    "console.print(\"‚è∞ This will take approximately 1-3 hours depending on your setup\")\n",
    "\n",
    "# Display current configuration\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "console.print(f\"\\n[bold blue]üìã Training Configuration:[/bold blue]\")\n",
    "console.print(f\"  Model: {config['model']['name']}\")\n",
    "console.print(f\"  Batch Size: {config['model']['batch_size']}\")\n",
    "console.print(f\"  Learning Rate: {config['model']['learning_rate']}\")\n",
    "console.print(f\"  Epochs: {config['model']['num_epochs']}\")\n",
    "console.print(f\"  Max Length: {config['model']['max_length']}\")\n",
    "console.print(f\"  Gradient Accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
    "\n",
    "# Confirm system readiness\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    console.print(f\"  GPU Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    console.print(\"  [yellow]‚ö†Ô∏è No GPU available - training will be slow[/yellow]\")\n",
    "\n",
    "console.print(\"\\n[yellow]üîÑ Starting training...[/yellow]\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the base model\n",
    "    model_path = train_base_model()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = (end_time - start_time) / 60  # Convert to minutes\n",
    "    \n",
    "    console.print(f\"\\n[bold green]‚úÖ Base model training completed![/bold green]\")\n",
    "    console.print(f\"[green]üìä Training time: {training_time:.1f} minutes[/green]\")\n",
    "    console.print(f\"[green]üìÅ Model saved to: {model_path}[/green]\")\n",
    "    \n",
    "    # Verify model was saved\n",
    "    if os.path.exists(model_path):\n",
    "        files = os.listdir(model_path)\n",
    "        console.print(f\"[blue]üì¶ Saved files: {files}[/blue]\")\n",
    "    \n",
    "    console.print(\"\\n[green]üéØ Ready for negative sample generation![/green]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    console.print(f\"\\n[red]‚ùå Training failed: {e}[/red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60cd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start base model training\n",
    "# This will:\n",
    "# 1. Run hyperparameter optimization (30 trials)\n",
    "# 2. Train final model with best parameters\n",
    "# 3. Save model and tokenizer\n",
    "\n",
    "console.print(\"üöÄ Starting base model training...\")\n",
    "console.print(\"‚è∞ This may take 2-4 hours depending on your setup\")\n",
    "\n",
    "base_trainer.train()\n",
    "\n",
    "console.print(\"‚úÖ Base model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from negative_sampler import generate_negative_samples\n",
    "from rich.console import Console\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Check if base model exists\n",
    "BASE_MODEL_PATH = \"./models/base/final\"\n",
    "if not os.path.exists(BASE_MODEL_PATH):\n",
    "    console.print(\"[red]‚ùå Base model not found! Please run base training first.[/red]\")\n",
    "    raise FileNotFoundError(\"Base model not found\")\n",
    "\n",
    "console.print(\"[bold blue]üé≠ Starting Negative Sample Generation[/bold blue]\")\n",
    "console.print(\"‚è∞ This will take approximately 30-60 minutes\")\n",
    "\n",
    "# Display configuration\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "console.print(f\"\\n[bold blue]üìã Generation Configuration:[/bold blue]\")\n",
    "console.print(f\"  Base Model: {BASE_MODEL_PATH}\")\n",
    "console.print(f\"  Batch Size: 2 (optimized for memory)\")\n",
    "console.print(f\"  Negatives per Sample: 3\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Generate negative samples\n",
    "    generate_negative_samples()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    generation_time = (end_time - start_time) / 60\n",
    "    \n",
    "    console.print(f\"\\n[bold green]‚úÖ Negative sample generation completed![/bold green]\")\n",
    "    console.print(f\"[green]üìä Generation time: {generation_time:.1f} minutes[/green]\")\n",
    "    \n",
    "    # Verify generated files\n",
    "    contrastive_dir = \"./data/contrastive\"\n",
    "    if os.path.exists(contrastive_dir):\n",
    "        files = os.listdir(contrastive_dir)\n",
    "        console.print(f\"[blue]üì¶ Generated files: {files}[/blue]\")\n",
    "        \n",
    "        # Show statistics\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(contrastive_dir, file)\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                console.print(f\"  {file}: {len(data)} samples\")\n",
    "    \n",
    "    console.print(\"\\n[green]üéØ Ready for contrastive learning training![/green]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    console.print(f\"\\n[red]‚ùå Negative sample generation failed: {e}[/red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import load_processed_data\n",
    "import os\n",
    "\n",
    "# Load processed data\n",
    "data = load_processed_data(\"./data/processed\")\n",
    "\n",
    "# Generate contrastive datasets\n",
    "os.makedirs(\"./data/contrastive\", exist_ok=True)\n",
    "\n",
    "console.print(\"üîÑ Generating negative samples...\")\n",
    "console.print(\"‚è∞ This may take 1-2 hours depending on dataset size\")\n",
    "\n",
    "for split in ['train', 'validation']:\n",
    "    if split in data:\n",
    "        console.print(f\"Processing {split} split...\")\n",
    "        \n",
    "        output_path = f\"./data/contrastive/{split}_contrastive.json\"\n",
    "        \n",
    "        contrastive_data = generator.generate_contrastive_dataset(\n",
    "            data[split],\n",
    "            output_path,\n",
    "            batch_size=8,\n",
    "            max_samples=None  # Set to smaller number for testing, e.g., 1000\n",
    "        )\n",
    "        \n",
    "        # Analyze quality\n",
    "        generator.analyze_negatives_quality(contrastive_data, sample_size=5)\n",
    "\n",
    "console.print(\"‚úÖ Negative sample generation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6054d04",
   "metadata": {},
   "source": [
    "## üîÑ Step 4: Contrastive Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start contrastive learning training\n",
    "# This will:\n",
    "# 1. Run hyperparameter optimization for Œª, Œ≥, k\n",
    "# 2. Train final model with contrastive loss + R-Drop\n",
    "# 3. Save final contrastive model\n",
    "\n",
    "console.print(\"üöÄ Starting contrastive learning training...\")\n",
    "console.print(\"‚è∞ This may take 1-3 hours\")\n",
    "\n",
    "contrastive_trainer.train()\n",
    "\n",
    "console.print(\"‚úÖ Contrastive learning training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc86a8",
   "metadata": {},
   "source": [
    "## üîÆ Step 5: Inference with Contrastive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b70eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import create_inference_engine\n",
    "from rich.console import Console\n",
    "import os\n",
    "import json\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Check if contrastive model exists\n",
    "contrastive_model_path = \"./models/contrastive/final\"\n",
    "base_model_path = \"./models/base/final\"\n",
    "\n",
    "if os.path.exists(contrastive_model_path):\n",
    "    model_path = contrastive_model_path\n",
    "    model_type = \"Contrastive Learning Model\"\n",
    "elif os.path.exists(base_model_path):\n",
    "    model_path = base_model_path\n",
    "    model_type = \"Base Model\"\n",
    "    console.print(\"[yellow]‚ö†Ô∏è Using base model (contrastive model not found)[/yellow]\")\n",
    "else:\n",
    "    console.print(\"[red]‚ùå No trained model found! Please run training first.[/red]\")\n",
    "    raise FileNotFoundError(\"No trained model found\")\n",
    "\n",
    "console.print(\"[bold blue]üîÆ Initializing Inference Engine[/bold blue]\")\n",
    "console.print(f\"[green]Using: {model_type}[/green]\")\n",
    "console.print(f\"[blue]Model Path: {model_path}[/blue]\")\n",
    "\n",
    "# Display inference configuration\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "console.print(f\"\\n[bold blue]üìã Inference Configuration:[/bold blue]\")\n",
    "console.print(f\"  Contrastive Search: {config['inference']['use_contrastive_search']}\")\n",
    "console.print(f\"  Alpha (Œ±): {config['inference']['contrastive_alpha']}\")\n",
    "console.print(f\"  K: {config['inference']['contrastive_k']}\")\n",
    "console.print(f\"  Beam Size: {config['inference']['num_beams']}\")\n",
    "\n",
    "try:\n",
    "    # Create inference engine\n",
    "    inference_engine = create_inference_engine(model_path)\n",
    "    \n",
    "    console.print(\"\\n[green]‚úÖ Inference engine initialized successfully![/green]\")\n",
    "    \n",
    "    # Test with a simple example\n",
    "    test_text = \"T√¥i ƒëi h·ªçc tr∆∞·ªùng ƒë·∫°i h·ªçc.\"\n",
    "    console.print(f\"\\n[yellow]üß™ Testing inference...[/yellow]\")\n",
    "    console.print(f\"Input: {test_text}\")\n",
    "    \n",
    "    corrected = inference_engine.correct_text(test_text)\n",
    "    console.print(f\"Output: {corrected}\")\n",
    "    \n",
    "    if corrected != test_text:\n",
    "        console.print(\"[green]‚úÖ Model is making corrections![/green]\")\n",
    "    else:\n",
    "        console.print(\"[blue]‚ÑπÔ∏è Model returned same text (may be already correct)[/blue]\")\n",
    "    \n",
    "    console.print(\"\\n[green]üéØ Ready for text correction![/green]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    console.print(f\"\\n[red]‚ùå Inference setup failed: {e}[/red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive correction (optional)\n",
    "# Uncomment to enable interactive mode\n",
    "\n",
    "# console.print(\"üéÆ Interactive mode - Enter text to correct (type 'quit' to exit):\")\n",
    "# contrastive_inference.interactive_correction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f1867",
   "metadata": {},
   "source": [
    "## üìä Step 6: Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d018b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_model import ModelEvaluator\n",
    "\n",
    "# Create model evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    model_path=CONTRASTIVE_MODEL_PATH,\n",
    "    data_dir=\"./data/processed\",\n",
    "    output_dir=\"./evaluation_results\"\n",
    ")\n",
    "\n",
    "console.print(\"üìä Model evaluator initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "console.print(\"üîç Starting comprehensive evaluation...\")\n",
    "console.print(\"‚è∞ This may take 30-60 minutes\")\n",
    "\n",
    "# Evaluate on test set with different decoding strategies\n",
    "evaluation_results = evaluator.evaluate_on_test_set(\n",
    "    max_samples=None,  # Set to smaller number for testing, e.g., 500\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "console.print(\"‚úÖ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error type analysis\n",
    "console.print(\"üî¨ Running error type analysis...\")\n",
    "\n",
    "error_analysis = evaluator.evaluate_error_types(\n",
    "    max_samples=1000  # Limit for faster analysis\n",
    ")\n",
    "\n",
    "console.print(\"‚úÖ Error type analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fabe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation visualizations\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Show evaluation comparison plot\n",
    "plot_path = \"./evaluation_results/evaluation_comparison.png\"\n",
    "if os.path.exists(plot_path):\n",
    "    console.print(\"üìà Evaluation Comparison Visualization:\")\n",
    "    display(Image(plot_path))\n",
    "else:\n",
    "    console.print(\"‚ùå Visualization not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show evaluation results summary\n",
    "import pandas as pd\n",
    "\n",
    "# Load and display comparison table\n",
    "csv_path = \"./evaluation_results/strategy_comparison.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    console.print(\"üìã Strategy Comparison Results:\")\n",
    "    display(df)\n",
    "else:\n",
    "    console.print(\"‚ùå Comparison table not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797276b8",
   "metadata": {},
   "source": [
    "## üìÅ Results and Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432def41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Pipeline Summary and Results\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "console = Console()\n",
    "\n",
    "console.print(\"[bold green]üéâ Vietnamese GEC Training Pipeline Completed![/bold green]\")\n",
    "\n",
    "# Check what was created\n",
    "results = {\n",
    "    \"base_model\": os.path.exists(\"./models/base/final\"),\n",
    "    \"contrastive_model\": os.path.exists(\"./models/contrastive/final\"),\n",
    "    \"processed_data\": os.path.exists(\"./data/processed\"),\n",
    "    \"contrastive_data\": os.path.exists(\"./data/contrastive\"),\n",
    "    \"inference_ready\": 'inference_engine' in globals()\n",
    "}\n",
    "\n",
    "# Create results table\n",
    "table = Table(title=\"Training Pipeline Results\")\n",
    "table.add_column(\"Component\", style=\"cyan\")\n",
    "table.add_column(\"Status\", style=\"green\")\n",
    "table.add_column(\"Location\", style=\"yellow\")\n",
    "\n",
    "table.add_row(\n",
    "    \"Base Model\",\n",
    "    \"‚úÖ Complete\" if results[\"base_model\"] else \"‚ùå Missing\",\n",
    "    \"./models/base/final\"\n",
    ")\n",
    "\n",
    "table.add_row(\n",
    "    \"Contrastive Model\", \n",
    "    \"‚úÖ Complete\" if results[\"contrastive_model\"] else \"‚ùå Missing\",\n",
    "    \"./models/contrastive/final\"\n",
    ")\n",
    "\n",
    "table.add_row(\n",
    "    \"Processed Data\",\n",
    "    \"‚úÖ Complete\" if results[\"processed_data\"] else \"‚ùå Missing\", \n",
    "    \"./data/processed\"\n",
    ")\n",
    "\n",
    "table.add_row(\n",
    "    \"Contrastive Data\",\n",
    "    \"‚úÖ Complete\" if results[\"contrastive_data\"] else \"‚ùå Missing\",\n",
    "    \"./data/contrastive\"\n",
    ")\n",
    "\n",
    "table.add_row(\n",
    "    \"Inference Engine\",\n",
    "    \"‚úÖ Ready\" if results[\"inference_ready\"] else \"‚ùå Not Ready\",\n",
    "    \"In Memory\"\n",
    ")\n",
    "\n",
    "console.print(table)\n",
    "\n",
    "# Model information\n",
    "if results[\"contrastive_model\"] or results[\"base_model\"]:\n",
    "    console.print(\"\\n[bold blue]üìä Model Information:[/bold blue]\")\n",
    "    \n",
    "    # Load configuration\n",
    "    with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    console.print(f\"  ü§ñ Base Model: {config['model']['name']}\")\n",
    "    console.print(f\"  üìè Max Length: {config['model']['max_length']}\")\n",
    "    console.print(f\"  üéØ Batch Size: {config['model']['batch_size']}\")\n",
    "    console.print(f\"  üìö Training Epochs: {config['model']['num_epochs']}\")\n",
    "    \n",
    "    if results[\"contrastive_model\"]:\n",
    "        console.print(f\"  üîÑ Contrastive Œª: {config['contrastive']['lambda_cl']}\")\n",
    "        console.print(f\"  üå°Ô∏è Temperature Œ≥: {config['contrastive']['temperature']}\")\n",
    "\n",
    "# File statistics\n",
    "console.print(f\"\\n[bold blue]üìÅ Generated Files:[/bold blue]\")\n",
    "\n",
    "def count_files_recursive(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        return 0, 0\n",
    "    \n",
    "    file_count = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        file_count += len(files)\n",
    "        for file in files:\n",
    "            try:\n",
    "                total_size += os.path.getsize(os.path.join(root, file))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return file_count, total_size\n",
    "\n",
    "dirs_to_check = [\n",
    "    (\"./models\", \"Models\"),\n",
    "    (\"./data\", \"Data\"),\n",
    "    (\"./logs\", \"Logs\")\n",
    "]\n",
    "\n",
    "for directory, name in dirs_to_check:\n",
    "    file_count, total_size = count_files_recursive(directory)\n",
    "    size_mb = total_size / (1024 * 1024)\n",
    "    console.print(f\"  üìÇ {name}: {file_count} files ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Usage instructions\n",
    "console.print(f\"\\n[bold green]üöÄ Quick Usage Guide:[/bold green]\")\n",
    "\n",
    "if results[\"inference_ready\"]:\n",
    "    console.print(\"‚úÖ Your model is ready to use! Try this:\")\n",
    "    console.print(\"\"\"\n",
    "# Correct Vietnamese text\n",
    "text = \"Your Vietnamese text here\"\n",
    "corrected = inference_engine.correct_text(text)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Corrected: {corrected}\")\n",
    "\"\"\")\n",
    "\n",
    "console.print(f\"\\n[bold blue]üí° Next Steps:[/bold blue]\")\n",
    "console.print(\"  üî∏ Test your model with more examples\")\n",
    "console.print(\"  üî∏ Fine-tune hyperparameters if needed\")\n",
    "console.print(\"  üî∏ Export model for production use\")\n",
    "console.print(\"  üî∏ Create evaluation metrics\")\n",
    "\n",
    "if results[\"contrastive_model\"]:\n",
    "    console.print(f\"\\n[bold green]üéä Congratulations![/bold green]\")\n",
    "    console.print(\"You have successfully trained a Vietnamese Grammatical Error Correction model\")\n",
    "    console.print(\"with Contrastive Learning! The model is ready for use.\")\n",
    "else:\n",
    "    console.print(f\"\\n[bold yellow]‚ö†Ô∏è Partial Completion[/bold yellow]\")\n",
    "    console.print(\"Some components may be missing. Check the status table above.\")\n",
    "\n",
    "# Performance tips\n",
    "console.print(f\"\\n[bold blue]‚ö° Performance Tips:[/bold blue]\")\n",
    "console.print(\"  üî∏ Use contrastive_search=True for better quality\")\n",
    "console.print(\"  üî∏ Use contrastive_search=False for faster inference\")\n",
    "console.print(\"  üî∏ Adjust alpha and k parameters for fine-tuning\")\n",
    "console.print(\"  üî∏ Batch process multiple texts for efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db072f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models and results (for local use)\n",
    "# Uncomment to create zip files for download\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# console.print(\"üì¶ Creating downloadable archives...\")\n",
    "\n",
    "# # Create zip files\n",
    "# shutil.make_archive('contrastive_gec_model', 'zip', './models/contrastive/final')\n",
    "# shutil.make_archive('evaluation_results', 'zip', './evaluation_results')\n",
    "\n",
    "# console.print(\"‚úÖ Archives created:\")\n",
    "# console.print(\"  üì¶ contrastive_gec_model.zip - Trained model\")\n",
    "# console.print(\"  üì¶ evaluation_results.zip - Evaluation results\")\n",
    "# console.print(\"\\nüíæ Use the file browser to download these files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab3d72",
   "metadata": {},
   "source": [
    "## üöÄ Quick Usage Guide\n",
    "\n",
    "Once training is complete, you can use the model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick usage example\n",
    "console.print(\"üöÄ [bold]Quick Usage Example:[/bold]\")\n",
    "\n",
    "# Load the model\n",
    "from inference import GECInference\n",
    "\n",
    "# Initialize\n",
    "gec_model = GECInference(\n",
    "    model_path=\"./models/contrastive/final\",\n",
    "    use_contrastive_search=True\n",
    ")\n",
    "\n",
    "# Correct text\n",
    "text = \"T√¥i ƒëi h·ªçc tr∆∞·ªùng ƒë·∫°i h·ªçc.\"\n",
    "corrected = gec_model.correct_text(text)\n",
    "\n",
    "console.print(f\"Original: {text}\")\n",
    "console.print(f\"Corrected: {corrected}\")\n",
    "\n",
    "console.print(\"\\nüí° [bold]Usage Tips:[/bold]\")\n",
    "console.print(\"  üéØ Use contrastive_search=True for better quality\")\n",
    "console.print(\"  ‚ö° Use contrastive_search=False for faster inference\")\n",
    "console.print(\"  üìä Adjust alpha and k parameters for fine-tuning\")\n",
    "console.print(\"  üìÅ Process files with correct_file() method\")\n",
    "\n",
    "# Practical Usage Examples and Export\n",
    "from rich.console import Console\n",
    "import json\n",
    "\n",
    "console = Console()\n",
    "\n",
    "console.print(\"[bold blue]üöÄ Practical Usage Examples[/bold blue]\")\n",
    "\n",
    "# Example 1: Single text correction\n",
    "console.print(\"\\n[yellow]Example 1: Single Text Correction[/yellow]\")\n",
    "example_code_1 = '''\n",
    "# Correct a single Vietnamese sentence\n",
    "text = \"T√¥i ƒëang h·ªçc ti·∫øng Vi·ªát ·ªü tr∆∞·ªùng ƒë·∫°i h·ªçc.\"\n",
    "corrected = inference_engine.correct_text(text)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Corrected: {corrected}\")\n",
    "'''\n",
    "console.print(example_code_1)\n",
    "\n",
    "if 'inference_engine' in globals():\n",
    "    text = \"T√¥i ƒëang h·ªçc ti·∫øng Vi·ªát ·ªü tr∆∞·ªùng ƒë·∫°i h·ªçc.\"\n",
    "    corrected = inference_engine.correct_text(text)\n",
    "    console.print(f\"‚úÖ [green]Original:[/green] {text}\")\n",
    "    console.print(f\"‚úÖ [green]Corrected:[/green] {corrected}\")\n",
    "\n",
    "# Example 2: Batch processing\n",
    "console.print(\"\\n[yellow]Example 2: Batch Processing[/yellow]\")\n",
    "example_code_2 = '''\n",
    "# Correct multiple texts at once\n",
    "texts = [\n",
    "    \"H√¥m nay tr·ªùi ƒë·∫πp qu√°.\",\n",
    "    \"Ch√∫ng ta c·∫ßn ph·∫£i h·ªçc b√†i n√†y.\",\n",
    "    \"T√¥i th√≠ch ƒÉn ph·ªü nhi·ªÅu l·∫Øm.\"\n",
    "]\n",
    "\n",
    "corrected_texts = inference_engine.correct_batch(texts)\n",
    "for original, corrected in zip(texts, corrected_texts):\n",
    "    print(f\"'{original}' ‚Üí '{corrected}'\")\n",
    "'''\n",
    "console.print(example_code_2)\n",
    "\n",
    "if 'inference_engine' in globals():\n",
    "    texts = [\n",
    "        \"H√¥m nay tr·ªùi ƒë·∫πp qu√°.\",\n",
    "        \"Ch√∫ng ta c·∫ßn ph·∫£i h·ªçc b√†i n√†y.\", \n",
    "        \"T√¥i th√≠ch ƒÉn ph·ªü nhi·ªÅu l·∫Øm.\"\n",
    "    ]\n",
    "    try:\n",
    "        corrected_texts = inference_engine.correct_batch(texts)\n",
    "        for original, corrected in zip(texts, corrected_texts):\n",
    "            console.print(f\"‚úÖ '{original}' ‚Üí '{corrected}'\")\n",
    "    except:\n",
    "        # Fallback to individual corrections\n",
    "        for text in texts:\n",
    "            corrected = inference_engine.correct_text(text)\n",
    "            console.print(f\"‚úÖ '{text}' ‚Üí '{corrected}'\")\n",
    "\n",
    "# Example 3: Configuration adjustment\n",
    "console.print(\"\\n[yellow]Example 3: Adjusting Inference Settings[/yellow]\")\n",
    "example_code_3 = '''\n",
    "# Modify inference settings for different scenarios\n",
    "import json\n",
    "\n",
    "# Load current config\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# For faster inference (lower quality)\n",
    "config['inference']['use_contrastive_search'] = False\n",
    "config['inference']['num_beams'] = 3\n",
    "\n",
    "# For better quality (slower)\n",
    "config['inference']['use_contrastive_search'] = True\n",
    "config['inference']['contrastive_alpha'] = 0.8\n",
    "config['inference']['contrastive_k'] = 6\n",
    "\n",
    "# Save updated config\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Recreate inference engine with new settings\n",
    "inference_engine = create_inference_engine(\"./models/contrastive/final\")\n",
    "'''\n",
    "console.print(example_code_3)\n",
    "\n",
    "# Export functions\n",
    "console.print(\"\\n[bold blue]üì¶ Model Export and Deployment[/bold blue]\")\n",
    "\n",
    "export_code = '''\n",
    "# Export model for deployment\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def export_model(model_path, export_name):\n",
    "    \"\"\"Export trained model as a zip file\"\"\"\n",
    "    shutil.make_archive(export_name, 'zip', model_path)\n",
    "    print(f\"Model exported as {export_name}.zip\")\n",
    "\n",
    "# Export the contrastive model\n",
    "if os.path.exists(\"./models/contrastive/final\"):\n",
    "    export_model(\"./models/contrastive/final\", \"vietnamese_gec_model\")\n",
    "\n",
    "# Export configuration\n",
    "shutil.copy(\"config.json\", \"vietnamese_gec_config.json\")\n",
    "print(\"Configuration exported as vietnamese_gec_config.json\")\n",
    "'''\n",
    "\n",
    "console.print(export_code)\n",
    "\n",
    "# Performance benchmarking\n",
    "console.print(\"\\n[bold blue]üìä Performance Benchmarking[/bold blue]\")\n",
    "\n",
    "if 'inference_engine' in globals():\n",
    "    import time\n",
    "    \n",
    "    # Benchmark inference speed\n",
    "    test_sentences = [\n",
    "        \"T√¥i ƒëi h·ªçc ·ªü tr∆∞·ªùng ƒë·∫°i h·ªçc.\",\n",
    "        \"H√¥m nay th·ªùi ti·∫øt r·∫•t ƒë·∫πp.\",\n",
    "        \"Ch√∫ng t√¥i s·∫Ω ƒëi du l·ªãch v√†o tu·∫ßn sau.\",\n",
    "        \"C√¥ ·∫•y l√† m·ªôt ng∆∞·ªùi r·∫•t th√¥ng minh.\",\n",
    "        \"Anh ·∫•y l√†m vi·ªác t·∫°i m·ªôt c√¥ng ty l·ªõn.\"\n",
    "    ]\n",
    "    \n",
    "    console.print(f\"‚è±Ô∏è Testing inference speed with {len(test_sentences)} sentences...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for sentence in test_sentences:\n",
    "        _ = inference_engine.correct_text(sentence)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    avg_time = total_time / len(test_sentences)\n",
    "    \n",
    "    console.print(f\"üìä Results:\")\n",
    "    console.print(f\"  Total time: {total_time:.2f} seconds\")\n",
    "    console.print(f\"  Average per sentence: {avg_time:.2f} seconds\")\n",
    "    console.print(f\"  Throughput: {len(test_sentences)/total_time:.1f} sentences/second\")\n",
    "\n",
    "console.print(f\"\\n[bold green]üéØ Your Vietnamese GEC model is ready for production use![/bold green]\")\n",
    "console.print(\"üí° Remember to save your work and download the model files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d2fae",
   "metadata": {},
   "source": [
    "## üìù Configuration and Hyperparameters\n",
    "\n",
    "Key hyperparameters used in this pipeline:\n",
    "\n",
    "### Base Training:\n",
    "- **Learning Rate**: Optimized via Optuna (typically 1e-5 to 1e-4)\n",
    "- **Label Smoothing**: 0.1\n",
    "- **Batch Size**: 8-32 (depending on GPU memory)\n",
    "- **Max Length**: 384 tokens\n",
    "- **Epochs**: 5-10\n",
    "\n",
    "### Contrastive Learning:\n",
    "- **Œª (lambda_cl)**: 1.0 (balance between CE and CL loss)\n",
    "- **Œ≥ (temperature)**: 0.25 (contrastive loss temperature)\n",
    "- **R-Drop Œ±**: 4.0 (R-Drop regularization strength)\n",
    "- **Epochs**: 3-5\n",
    "\n",
    "### Contrastive Search:\n",
    "- **Œ± (alpha)**: 0.7 (balance between confidence and diversity)\n",
    "- **k**: 5 (top-k candidates)\n",
    "- **Beam Size**: 1 (as recommended in paper)\n",
    "\n",
    "These parameters can be adjusted based on your specific needs and computational resources."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
